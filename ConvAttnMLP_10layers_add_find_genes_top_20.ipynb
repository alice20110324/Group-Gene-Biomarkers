{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e3cba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ext_disk/zhenfang/davis/results/model1/without_cross/3/full_model_2_50.pt\n",
      "/media/ext_disk/zhenfang/davis/results/model1/without_cross/3/results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "root_path = '/media/ext_disk/zhenfang/davis/results/model1/without_cross/'\n",
    "sub_path = '2/'\n",
    "model_file = 'full_model_2_50.pt'\n",
    "\n",
    "#results_save_file='/media/ext_disk/zhenfang/davis/results/model1/without_cross/2/results.txt'\n",
    "results_file='results.txt'\n",
    "\n",
    "\n",
    "\n",
    "sub_path=str(int(sub_path[:-1])+1)+'/'\n",
    "        \n",
    "model_path=os.path.join(root_path, sub_path, model_file)\n",
    "results_save_file=os.path.join(root_path,sub_path,results_file)\n",
    "\n",
    "#results_save_file=os.path.join(root_path,sub_path,model_file)\n",
    "\n",
    "\n",
    "print(model_path)\n",
    "print(results_save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba621bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import torchmetrics\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549530c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttMLP_10layers_add__(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 1100)\n",
    "        self.fc11=nn.Linear(1100,1100)\n",
    "        self.bn1 = nn.BatchNorm1d(1100)\n",
    "        self.fc2 = nn.Linear(1100, 100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 9)\n",
    "        self.bn3 = nn.BatchNorm1d(9)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(16, 16, 5, padding=2)  # Change to match input channel 16\n",
    "        self.conv4 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        #self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(3300)\n",
    "        #self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        self.bn5 = nn.BatchNorm1d(3300)\n",
    "        self.pool2 = nn.MaxPool1d(3, stride=3)\n",
    "        #self.conv3 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        self.pool3 = nn.MaxPool1d(3, stride=3)\n",
    "        self.bn6 = nn.BatchNorm1d(1100)\n",
    "        \n",
    "        self.bn7=nn.BatchNorm1d(1100)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=3300)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=3300)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv1d(16, 16 // 8, 1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16 // 8, 16, 1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define a list to hold multiple layers\n",
    "        self.layers = nn.ModuleList([self._create_layer() for _ in range(2)])\n",
    "\n",
    "    def _create_layer(self):\n",
    "        \"\"\"Create a single layer using conv and pooling operations as defined in the original forward method.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 5, padding=2),\n",
    "            nn.BatchNorm1d(3300),\n",
    "            nn.Conv1d(8, 16, 5, padding=2),\n",
    "            nn.Conv1d(16, 16, 5, padding=2),  # Match input/output channel\n",
    "            nn.Conv1d(16, 1, 1, stride=1),  # This will receive 16 channels and output 1\n",
    "            nn.AvgPool1d(kernel_size=3300),\n",
    "            nn.MaxPool1d(kernel_size=3300)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x1= F.relu(self.drop(self.bn1(self.fc11(self.fc1(x)))))\n",
    "        \n",
    "        \n",
    "        x0 = torch.unsqueeze(x, 1)  # Add a channel dimension\n",
    "        #print('x0:',x0.shape)\n",
    "        # Pass through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            x2 = F.relu(layer[0](x0))  # Conv1\n",
    "            x2 = F.relu(layer[2](x2))  # Conv2\n",
    "            avg_pool = self.avg_pool(x2)\n",
    "            avg_out = self.fc(self.avg_pool(x2))\n",
    "            max_pool = self.max_pool(x2)\n",
    "            max_out = self.fc(self.max_pool(x2))\n",
    "            out = avg_out + max_out\n",
    "            x2 = torch.mul(x2, out)\n",
    "            x2 = F.relu(layer[4](x2))  # Conv3\n",
    "            x2 = x0 + x2  # Skip connection\n",
    "            x0 = x2  # Pass to next layer\n",
    "            #print('x0+++:',x0.shape)\n",
    "            \n",
    "        x2=self.pool3(x2)\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=torch.squeeze(x2,1)\n",
    "        x2=self.bn6(x2)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #x3=torch.concat([x1,x2],dim=1)\n",
    "        #print('x2_final:',x2.shape)\n",
    "        x3=torch.add(x1,x2)\n",
    "        #x3=torch.cat([x1,x2],dim=1)\n",
    "        x4=self.bn7(x3)\n",
    "        x4 = F.relu(self.drop(self.bn2(self.fc2(x4))))\n",
    "        return F.softmax(self.bn3(self.fc3(x4)), dim=1) \n",
    "    \n",
    "#modela=ConvAttMLP_10layers_add__()\n",
    "#modela.cuda()\n",
    "#print(modela)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab17d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttMLP_10layers_add__LRP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.bn0 = nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 1100)\n",
    "        self.fc11=nn.Linear(1100,1100)\n",
    "        #self.bn1 = nn.BatchNorm1d(1100)\n",
    "        self.fc2 = nn.Linear(1100, 100)\n",
    "        #self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 9)\n",
    "        #self.bn3 = nn.BatchNorm1d(9)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(16, 16, 5, padding=2)  # Change to match input channel 16\n",
    "        self.conv4 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        #self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        #self.bn4 = nn.BatchNorm1d(3300)\n",
    "        #self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        #self.bn5 = nn.BatchNorm1d(3300)\n",
    "        self.pool2 = nn.MaxPool1d(3, stride=3)\n",
    "        #self.conv3 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        self.pool3 = nn.MaxPool1d(3, stride=3)\n",
    "        #self.bn6 = nn.BatchNorm1d(1100)\n",
    "        \n",
    "        #self.bn7=nn.BatchNorm1d(1100)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=3300)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=3300)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv1d(16, 16 // 8, 1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16 // 8, 16, 1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define a list to hold multiple layers\n",
    "        self.layers = nn.ModuleList([self._create_layer() for _ in range(2)])\n",
    "\n",
    "    def _create_layer(self):\n",
    "        \"\"\"Create a single layer using conv and pooling operations as defined in the original forward method.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 5, padding=2),\n",
    "            nn.BatchNorm1d(3300),\n",
    "            nn.Conv1d(8, 16, 5, padding=2),\n",
    "            nn.Conv1d(16, 16, 5, padding=2),  # Match input/output channel\n",
    "            nn.Conv1d(16, 1, 1, stride=1),  # This will receive 16 channels and output 1\n",
    "            nn.AvgPool1d(kernel_size=3300),\n",
    "            nn.MaxPool1d(kernel_size=3300)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.bn0(x)\n",
    "        x1= F.relu(self.drop(self.bn1(self.fc11(self.fc1(x)))))\n",
    "        \n",
    "        '''\n",
    "        x0 = torch.unsqueeze(x, 1)  # Add a channel dimension\n",
    "        #print('x0:',x0.shape)\n",
    "        # Pass through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            x2 = F.relu(layer[0](x0))  # Conv1\n",
    "            x2 = F.relu(layer[2](x2))  # Conv2\n",
    "            avg_pool = self.avg_pool(x2)\n",
    "            avg_out = self.fc(self.avg_pool(x2))\n",
    "            max_pool = self.max_pool(x2)\n",
    "            max_out = self.fc(self.max_pool(x2))\n",
    "            out = avg_out + max_out\n",
    "            x2 = torch.mul(x2, out)\n",
    "            x2 = F.relu(layer[4](x2))  # Conv3\n",
    "            x2 = x0 + x2  # Skip connection\n",
    "            x0 = x2  # Pass to next layer\n",
    "            #print('x0+++:',x0.shape)\n",
    "           \n",
    "        x2=self.pool3(x2)\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=torch.squeeze(x2,1)\n",
    "        x2=self.bn6(x2)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #x3=torch.concat([x1,x2],dim=1)\n",
    "        #print('x2_final:',x2.shape)\n",
    "        '''\n",
    "        x3=torch.add(x1,x2)\n",
    "        #x3=torch.cat([x1,x2],dim=1)\n",
    "        x4=self.bn7(x3)\n",
    "        x4 = F.relu(self.drop(self.bn2(self.fc2(x4))))\n",
    "        return F.softmax(self.bn3(self.fc3(x4)), dim=1) \n",
    "    \n",
    "#modela=ConvAttMLP_10layers_add__()\n",
    "#modela.cuda()\n",
    "#print(modela)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec9852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAttMLP_10layers_add(\n",
      "  (bn0): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3300, out_features=1100, bias=True)\n",
      "  (fc11): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "  (bn1): BatchNorm1d(1100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=2200, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (conv1): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv4): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn6): BatchNorm1d(1100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn7): BatchNorm1d(2200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avg_pool): AvgPool1d(kernel_size=(3300,), stride=(3300,), padding=(0,))\n",
      "  (max_pool): MaxPool1d(kernel_size=3300, stride=3300, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Sequential(\n",
      "    (0): Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(2, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (4): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "      (5): AvgPool1d(kernel_size=(3300,), stride=(3300,), padding=(0,))\n",
      "      (6): MaxPool1d(kernel_size=3300, stride=3300, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (1): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (4): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
      "      (5): AvgPool1d(kernel_size=(3300,), stride=(3300,), padding=(0,))\n",
      "      (6): MaxPool1d(kernel_size=3300, stride=3300, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvAttMLP_10layers_add(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 1100)\n",
    "        self.fc11=nn.Linear(1100,1100)\n",
    "        self.bn1 = nn.BatchNorm1d(1100)\n",
    "        self.fc2 = nn.Linear(2200, 100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 9)\n",
    "        self.bn3 = nn.BatchNorm1d(9)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(16, 16, 5, padding=2)  # Change to match input channel 16\n",
    "        self.conv4 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        #self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(3300)\n",
    "        #self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        self.bn5 = nn.BatchNorm1d(3300)\n",
    "        self.pool2 = nn.MaxPool1d(3, stride=3)\n",
    "        #self.conv3 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        self.pool3 = nn.MaxPool1d(3, stride=3)\n",
    "        self.bn6 = nn.BatchNorm1d(1100)\n",
    "        \n",
    "        self.bn7=nn.BatchNorm1d(2200)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=3300)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=3300)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv1d(16, 16 // 8, 1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16 // 8, 16, 1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define a list to hold multiple layers\n",
    "        self.layers = nn.ModuleList([self._create_layer() for _ in range(2)])\n",
    "\n",
    "    def _create_layer(self):\n",
    "        \"\"\"Create a single layer using conv and pooling operations as defined in the original forward method.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 5, padding=2),\n",
    "            nn.BatchNorm1d(3300),\n",
    "            nn.Conv1d(8, 16, 5, padding=2),\n",
    "            nn.Conv1d(16, 16, 5, padding=2),  # Match input/output channel\n",
    "            nn.Conv1d(16, 1, 1, stride=1),  # This will receive 16 channels and output 1\n",
    "            nn.AvgPool1d(kernel_size=3300),\n",
    "            nn.MaxPool1d(kernel_size=3300)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x1= F.relu(self.drop(self.bn1(self.fc11(self.fc1(x)))))\n",
    "        \n",
    "        \n",
    "        x0 = torch.unsqueeze(x, 1)  # Add a channel dimension\n",
    "        #print('x0:',x0.shape)\n",
    "        # Pass through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            x2 = F.relu(layer[0](x0))  # Conv1\n",
    "            x2 = F.relu(layer[2](x2))  # Conv2\n",
    "            avg_pool = self.avg_pool(x2)\n",
    "            avg_out = self.fc(self.avg_pool(x2))\n",
    "            max_pool = self.max_pool(x2)\n",
    "            max_out = self.fc(self.max_pool(x2))\n",
    "            out = avg_out + max_out\n",
    "            x2 = torch.mul(x2, out)\n",
    "            x2 = F.relu(layer[4](x2))  # Conv3\n",
    "            x2 = x0 + x2  # Skip connection\n",
    "            x0 = x2  # Pass to next layer\n",
    "            #print('x0+++:',x0.shape)\n",
    "            \n",
    "        x2=self.pool3(x2)\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=torch.squeeze(x2,1)\n",
    "        x2=self.bn6(x2)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #x3=torch.concat([x1,x2],dim=1)\n",
    "        #print('x2_final:',x2.shape)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        x3=torch.cat([x1,x2],dim=1)\n",
    "        x4=self.bn7(x3)\n",
    "        x4 = F.relu(self.drop(self.bn2(self.fc2(x4))))\n",
    "        return F.softmax(self.bn3(self.fc3(x4)), dim=1) \n",
    "    \n",
    "modela=ConvAttMLP_10layers_add()\n",
    "modela.cuda()\n",
    "print(modela)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c2244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAttMLP_10layers_add_LRP(\n",
      "  (fc1): Linear(in_features=3300, out_features=1100, bias=True)\n",
      "  (fc11): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "  (fc22): Linear(in_features=1100, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvAttMLP_10layers_add_LRP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.bn0 = nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 1100)\n",
    "        self.fc11=nn.Linear(1100,1100)\n",
    "        #self.bn1 = nn.BatchNorm1d(1100)\n",
    "        #self.fc2 = nn.Linear(2200, 100)\n",
    "        self.fc22=nn.Linear(1100,100)\n",
    "        #self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 9)\n",
    "        #self.bn3 = nn.BatchNorm1d(9)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        # Conv layers\n",
    "        #self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        #self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        #self.conv3 = nn.Conv1d(16, 16, 5, padding=2)  # Change to match input channel 16\n",
    "        #self.conv4 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        #self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        #self.bn4 = nn.BatchNorm1d(3300)\n",
    "        #self.conv2 = nn.Conv1d(8, 16, 5, padding=2)\n",
    "        #self.bn5 = nn.BatchNorm1d(3300)\n",
    "        #self.pool2 = nn.MaxPool1d(3, stride=3)\n",
    "        #self.conv3 = nn.Conv1d(16, 1, 1, stride=1)\n",
    "        #self.pool3 = nn.MaxPool1d(3, stride=3)\n",
    "        #self.bn6 = nn.BatchNorm1d(1100)\n",
    "        \n",
    "        #self.bn7=nn.BatchNorm1d(2200)\n",
    "        '''\n",
    "        self.avg_pool = nn.AvgPool1d(kernel_size=3300)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=3300)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv1d(16, 16 // 8, 1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16 // 8, 16, 1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Define a list to hold multiple layers\n",
    "        self.layers = nn.ModuleList([self._create_layer() for _ in range(2)])\n",
    "        '''\n",
    "        \n",
    "    ''' \n",
    "    def _create_layer(self):\n",
    "        \"\"\"Create a single layer using conv and pooling operations as defined in the original forward method.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 5, padding=2),\n",
    "            nn.BatchNorm1d(3300),\n",
    "            nn.Conv1d(8, 16, 5, padding=2),\n",
    "            nn.Conv1d(16, 16, 5, padding=2),  # Match input/output channel\n",
    "            nn.Conv1d(16, 1, 1, stride=1),  # This will receive 16 channels and output 1\n",
    "            nn.AvgPool1d(kernel_size=3300),\n",
    "            nn.MaxPool1d(kernel_size=3300)\n",
    "        )\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        print('x:',x.shape)\n",
    "        #x2=x[0,3300:]\n",
    "        #x = self.bn0(x)\n",
    "        #print('x_bn0:',x.shape)\n",
    "        #x2=torch.zeros([1,1100])\n",
    "        #x2=x2.cuda()\n",
    "        #xm=torch.cat([x,x2],dim=1)\n",
    "        \n",
    "        #print('xm.shape:',xm.shape)\n",
    "        x1= F.relu(self.drop(self.fc11(self.fc1(x))))\n",
    "        \n",
    "        \"\"\"\n",
    "        x0 = torch.unsqueeze(x, 1)  # Add a channel dimension\n",
    "        #print('x0:',x0.shape)\n",
    "        # Pass through each layer sequentially\n",
    "        for layer in self.layers:\n",
    "            ）\n",
    "            x2 = F.relu(layer[0](x0))  # Conv1\n",
    "            x2 = F.relu(layer[2](x2))  # Conv2\n",
    "            avg_pool = self.avg_pool(x2)\n",
    "            avg_out = self.fc(self.avg_pool(x2))\n",
    "            max_pool = self.max_pool(x2)\n",
    "            max_out = self.fc(self.max_pool(x2))\n",
    "            out = avg_out + max_out\n",
    "            x2 = torch.mul(x2, out)\n",
    "            x2 = F.relu(layer[4](x2))  # Conv3\n",
    "            x2 = x0 + x2  # Skip connection\n",
    "            x0 = x2  # Pass to next layer\n",
    "            #print('x0+++:',x0.shape)\n",
    "            \n",
    "        x2=self.pool3(x2)\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=torch.squeeze(x2,1)\n",
    "        x2=self.bn6(x2)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #x3=torch.concat([x1,x2],dim=1)\n",
    "        #print('x2_final:',x2.shape)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        \"\"\"\n",
    "        #print('x1.device:',x1.device)\n",
    "        #x2=torch.zeros([1,1100])\n",
    "        #x2=x2.cuda()\n",
    "        #print('x2.device:',x2.device)\n",
    "        #print('x1.shape:',x1.shape)\n",
    "        #print('x2.shape:',x2.shape)\n",
    "        #x2=torch.squeeze(x2,0)\n",
    "        #print('x2.shape:',x2.shape)\n",
    "        #x3=torch.cat([x1,x2],dim=-1)\n",
    "        #print('x3.shape:',x3.shape)\n",
    "        #x4=self.bn7(x3)\n",
    "        x4 = F.relu(self.drop(self.fc22(x1)))\n",
    "        return F.softmax(self.fc3(x4), dim=-1) \n",
    "    \n",
    "modela=ConvAttMLP_10layers_add_LRP()\n",
    "modela.cuda()\n",
    "print(modela)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57115fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttMLP_LRP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 1100)\n",
    "        self.bn1= nn.BatchNorm1d(1100)\n",
    "        self.fc2 = nn.Linear(1100, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        #self.pool1 = nn.MaxPool1d(3,stride=3)\n",
    "        self.bn4=nn.BatchNorm1d(3300)\n",
    "        self.conv2 = nn.Conv1d(8, 16, 5,padding=2)\n",
    "        self.bn5=nn.BatchNorm1d(3300)\n",
    "        self.pool2 = nn.MaxPool1d(3,stride=3)\n",
    "        self.conv3=nn.Conv1d(16,1,1,stride=1)\n",
    "        self.pool3=nn.MaxPool1d(3,stride=3)\n",
    "        self.bn6=nn.BatchNorm1d(1100)\n",
    "        #self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        #self.fc2 = nn.Linear(120, 84)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "        self.bn7=nn.BatchNorm1d(1100)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv1d(16, 16 // 8, 1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16 // 8, 16, 1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #x=self.bn0(x)\n",
    "        #print('x:',x.shape)\n",
    "        x1= F.relu(self.drop(self.fc1(x)))\n",
    "        \n",
    "        '''\n",
    "        x0=torch.unsqueeze(x,1)\n",
    "        print(\"x0:\",x0.shape)\n",
    "        \n",
    "        \n",
    "        x2=F.relu(self.conv1(x0))\n",
    "        #x2=x2+x0\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=F.relu(self.conv2(x2))\n",
    "        #x2=x2+x0\n",
    "        #print('x2:',x2.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        avg_out = self.fc(self.avg_pool(x2))\n",
    "        max_out = self.fc(self.max_pool(x2))\n",
    "        out = avg_out + max_out\n",
    "        x2=torch.mul(x2,out)\n",
    "        x2=F.relu(self.conv3(x2))\n",
    "        print('x2:',x2.shape)\n",
    "        #x2=x0+x2\n",
    "        \n",
    "        #print('x2:',x2.shape)\n",
    "        #x2=x0+x2####skip connection\n",
    "        #x2=torch.squeeze(x2,1)\n",
    "        x2=self.bn5(x2)\n",
    "        #x2 = torch.unsqueeze(x2, 1)  # 在第 1 维（通道维）添加维度#####################################\n",
    "        #x2 = self.pool3(x2)  # 现在可以通过 MaxPool1d 了\n",
    "        x2=self.pool3(x2)\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=self.bn6(x2)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #x3=torch.concat([x1,x2],dim=1)\n",
    "        '''\n",
    "        x2=torch.zeros([1,1100])\n",
    "        x3=torch.add(x1,x2)\n",
    "        #x4=self.bn7(x3)\n",
    "        x4 = F.relu(self.drop(self.fc2(x3)))\n",
    "        return F.softmax(self.fc3(x4), dim=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec2ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP1(\n",
      "  (bn0): BatchNorm1d(237, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=237, out_features=2000, bias=True)\n",
      "  (bn1): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=2000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#import Trainer\n",
    "#from network import NFM\n",
    "import torch.utils.data as Data\n",
    "#from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "#import torchmetrics\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':2000,\n",
    "    #'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    #'embed_input_dim':1001,#embed输入维度\n",
    "    #'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    #'train_data':'dataset/qiuguan/encode/encode_1000/train/train_encode_data_1000_new.csv',\n",
    "    #'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    #'guan_test_data':'dataset/qiuguan/non_code/guan_test/guan_test_data.csv',\n",
    "    #'guan_test_label':'dataset/qiuguan/non_code/guan_test/guan_test_label.csv',\n",
    "    #'test_data':'dataset/qiuguan/encode/encode_1000/test/test_encode_data_1000_new.csv',\n",
    "    #'test_label':'dataset/qiuguan/non_code/test/test_labels.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model = MLP().cuda()\n",
    "#print(model)\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(237)\n",
    "        self.fc1 = nn.Linear(237, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model1 = MLP1().cuda()\n",
    "print(model1)\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(126)\n",
    "        self.fc1 = nn.Linear(126, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model2 = MLP2().cuda()\n",
    "#print(model2)\n",
    "\n",
    "\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(177)\n",
    "        self.fc1 = nn.Linear(177, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model3 = MLP3().cuda()\n",
    "#print(model3)\n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(130)\n",
    "        self.fc1 = nn.Linear(130, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model4 = MLP4().cuda()\n",
    "\n",
    "class MLP5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(172)\n",
    "        self.fc1 = nn.Linear(172, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model5 = MLP5().cuda()\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(model4)\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "#import config\n",
    "#import evaluate\n",
    "#import data_utils\n",
    "#import Trainer\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "class KZDatasetPredict(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, df_list):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(df_list)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data,label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "   \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,df_list):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        #print('data_info:',df_list[-1])\n",
    "        #df=pd.read_csv(csv_path,sep=',')\n",
    "        #df=df.iloc[:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        label=int(df_list[-1])\n",
    "        labels.append(label)\n",
    "        #print('labels:',labels)\n",
    "        data=df_list[:-1]\n",
    "        #df_np=np.array(df_list)\n",
    "        #print(rows,cols)\n",
    "        \n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels.shape:',labels.shape)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        \n",
    "           \n",
    "        \n",
    "        data=np.array(data)#\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        data=torch.from_numpy(data)#\n",
    "            \n",
    "        labels=torch.from_numpy(labels)#\n",
    "        #bi_data=embding_process(nfm_config,data)\n",
    "        #print(\"bi_data.shape:\",bi_data.shape)\n",
    "            \n",
    "            \n",
    "        data_info.append((data,label))\n",
    "        return data_info\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "   \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        df=df.iloc[:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "       \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        df=df.iloc[:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        \n",
    "        print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "#import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import torchmetrics\n",
    "            \n",
    "\n",
    "from torchmetrics.classification import accuracy\n",
    "\n",
    "def train_epoch(model,train_loader,batch_size,optimizer,loss_func):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "            \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]\n",
    "            \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        y_predict = model(x)\n",
    "            \n",
    "        loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        #\n",
    "        '''\n",
    "        train_acc_en=torchmetrics.Accuracy(task='multiclass',num_classes=9).cuda()\n",
    "        batch_train_acc=train_acc_en(y_predict,labels_int)\n",
    "        '''\n",
    "        #batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        #batch_train_acc=torchmetrics.classification.Accuracy(y_predict,labels_int)\n",
    "        batch_train_acc = torchmetrics.functional.accuracy(y_predict, labels_int, task='multiclass', num_classes=nfm_config['n_class'])\n",
    "        \n",
    "        \n",
    "        #train_acc_en=accuracy(y_predict,labels_int)\n",
    "        #batch_train_acc=train_acc_en()\n",
    "        \n",
    "        total_train_accuracy+=batch_train_acc\n",
    "            \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "\n",
    "def val_epoch(model,test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "           \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #test_acc_en=torchmetrics.Accuracy(task='multiclass',num_classes=9).cuda()\n",
    "            #batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            batch_test_acc = torchmetrics.functional.accuracy(yhat, targets, task='multiclass', num_classes=nfm_config['n_class'])\n",
    "            total_test_acc+=batch_test_acc\n",
    "            \n",
    "            batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d08d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the means for the predicted correctly\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import accuracy_score \n",
    " \n",
    "def bool_predict_model(test_dl, model): \n",
    "    model.eval()#测试数据稳定\n",
    "    error_dataset=torch.tensor([0])\n",
    "    error_dataset=error_dataset.view(1,1)\n",
    "    predictions, actuals = [], [] \n",
    "    num=0\n",
    "    for i, (inputs,targets) in enumerate(test_dl): \n",
    "        # evaluate the model on the test set \n",
    "        #print(\\ inputs:\\ ,inputs) \n",
    "        #print(\\ targets:\\ ,targets) \n",
    "        inputs = Variable(inputs) \n",
    "        #bi_inputs=Variable(bi_inputs)\n",
    "        targets = Variable(targets) \n",
    "        #print(targets)\n",
    "        #print('targets:',targets.shape)        \n",
    "        #targets=targets.argmax(axis=1)  \n",
    "        #print('targets:',targets.shape)\n",
    "        #x = torch.tensor(x, dtype=torch.float) \n",
    "        #x=x.clone().detach().requires_grad_(True) \n",
    "        inputs=torch.tensor(inputs,dtype=torch.float) \n",
    "        #bi_inputs=torch.tensor(bi_inputs,dtype=torch.float)\n",
    "        targets=torch.tensor(targets,dtype=torch.float) \n",
    "        inputs, targets = inputs.cuda(),targets.cuda() \n",
    "        yhat = model(inputs) \n",
    "        \n",
    "        yhat=yhat.argmax(axis=1)\n",
    "        #print('yhat:',yhat.shape)\n",
    "        \n",
    "        if targets==yhat:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "#find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097bc483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26270/4283510531.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs=torch.tensor(inputs,dtype=torch.float)\n",
      "/tmp/ipykernel_26270/4283510531.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets=torch.tensor(targets,dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 3301)\n"
     ]
    }
   ],
   "source": [
    "#find the samples that predicted correctly\n",
    "import pandas as pd \n",
    "test_df=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/test_info1.csv',sep=',')\n",
    "test_df=test_df.iloc[:,1:]\n",
    "rows,cols=test_df.shape\n",
    "#print(rows,cols)\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='models/ConvAttMLP_10layers_add//MLP310.pt'\n",
    "#path='models/ConvAttMLP_res_concat/MLP410.pt'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "#mlp=MLP()\n",
    "model=ConvAttMLP_10layers_add()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(path), strict=False)\n",
    "'''\n",
    "# 输出模型的参数名和参数形状\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter Name: {name}, Shape: {param.size()}\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "model_lrp=ConvAttMLP_LRP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "# Load the state_dict for both models separately\n",
    "model.load_state_dict(torch.load(path), strict=False)\n",
    "model_lrp.load_state_dict(model.state_dict())  # Copying the loaded state dict to model_lrp\n",
    "\n",
    "model.cuda()\n",
    "model_lrp.cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_params = list(model.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=model\n",
    "'''\n",
    "\n",
    "#testset = KZDatasetPredict(test_df)\n",
    "''''\n",
    "test_loader = data.DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "'''\n",
    "false_list=[]\n",
    "for i in range(rows):\n",
    "    df_list=test_df.iloc[i,:].tolist()\n",
    "    #print(type(df_list))\n",
    "    #print('df_list:',df_list[-1])\n",
    "    #print(data_set)\n",
    "    #print('this is data_set')\n",
    "    data_set = KZDatasetPredict(df_list)\n",
    "    data_test_loader=data.DataLoader(dataset=data_set,\n",
    "                                    batch_size=1)\n",
    "    #print(data_test_loader)\n",
    "    bool_index=bool_predict_model(data_test_loader,model)\n",
    "    #print(bool_index)\n",
    "    if bool_index==False:\n",
    "        \n",
    "        false_list.append(i)\n",
    "        #test_df=test_df.drop(index=i, inplace=True)\n",
    "        \n",
    "for i,aitem in enumerate(false_list):\n",
    "    test_df.drop(index=i, inplace=True)\n",
    "print(test_df.shape)   \n",
    "\n",
    "test_df.to_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/ConvAttMLP_10layers_add//test_info3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d3d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26270/1989142859.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  row_mean_tensor=torch.tensor(row_mean_tensor,dtype=torch.float)###################################\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.91 GiB total capacity; 68.35 MiB already allocated; 5.75 MiB free; 84.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minnvestigator2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InnvestigateModel\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#from utils import Flatten\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/nn/modules/module.py:542\u001b[0m, in \u001b[0;36mModule.double\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdouble\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/nn/modules/module.py:387\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 387\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    391\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    398\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/nn/modules/module.py:409\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 409\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/nn/modules/module.py:542\u001b[0m, in \u001b[0;36mModule.double.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdouble\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Casts all floating point parameters and buffers to ``double`` datatype.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 11.91 GiB total capacity; 68.35 MiB already allocated; 5.75 MiB free; 84.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#top_20\n",
    "\n",
    "\n",
    "#compute mean value of all the correctly predicted samples:\n",
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "test_df=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/ConvAttMLP_10layers_add//test_info3.csv',sep=',')\n",
    "test_df=test_df.iloc[:,1:]\n",
    "rows,cols=test_df.shape\n",
    "#print(rows,cols)\n",
    "columns=test_df.columns[:-1].tolist()\n",
    "#columns=torch.tensor(columns).cuda()#ValueError: too many dimensions 'str'\n",
    "\n",
    "\n",
    "row_mean=test_df.mean(axis=0)\n",
    "#print(row_mean)\n",
    "row_mean=row_mean[:-1]#drop off label\n",
    "row_mean_np=np.array(row_mean).reshape(1,-1)\n",
    "#print(row_mean_np)\n",
    "row_mean_df=pd.DataFrame(row_mean_np)\n",
    "#print(row_mean_df)\n",
    "row_mean_df.columns=test_df.columns[:-1]\n",
    "#print(row_mean_df)\n",
    "row_mean_tensor=torch.from_numpy(row_mean_np).cuda()\n",
    "row_mean_tensor=torch.tensor(row_mean_tensor,dtype=torch.float)###################################\n",
    "#去掉行\n",
    "#print('row_mean_tensor:',row_mean_tensor.shape)\n",
    "\n",
    "row_mean_tensor.cuda()\n",
    "#print('x_bn0:',x.shape)\n",
    "#x2=torch.zeros([1,1100])\n",
    "#x2=x2.cuda()\n",
    "#xm=torch.cat([row_mean_tensor,x2],dim=1)\n",
    "\n",
    "#compute contribution using the Equ.\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='models/ConvAttMLP_10layers_add/MLP310.pt'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "model=ConvAttMLP_10layers_add()\n",
    "model_lrp=ConvAttMLP_10layers_add_LRP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "# Load the state_dict for both models separately\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(path), strict=False)\n",
    "'''\n",
    "# 将原始网络的参数赋值给子网络\n",
    "for param_name, param in model.named_parameters():\n",
    "    if hasattr(model_lrp, param_name):\n",
    "        sub_param = getattr(model_lrp, param_name)\n",
    "        if sub_param.data.shape == param.data.shape:\n",
    "            sub_param.data = param.data.clone()\n",
    "'''            \n",
    "#model_lrp.named_parameters['fc22.weight']=model.named_parameters['fc2.weight'][:,:1100]\n",
    "#model_lrp.named_parameters['fc22.bias']=model.named_parameters['fc2.bias']\n",
    "model_lrp.fc22.weight.data=model.fc2.weight.data[:,:1100]\n",
    "\n",
    "'''            \n",
    "# 输出模型的参数名和参数形状\n",
    "for name, param in model_lrp.named_parameters():\n",
    "    print(f\"Parameter Name: {name}, Shape: {param.size()}\")\n",
    "'''            \n",
    "            \n",
    "#model_lrp.load_state_dict(model.state_dict())  # Copying the loaded state dict to model_lrp\n",
    "\n",
    "model.cuda()\n",
    "model_lrp.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    #n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator2 import InnvestigateModel\n",
    "#from utils import Flatten\n",
    "model.double()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "inn_model = InnvestigateModel(model_lrp, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "genes_features=np.array([i for i  in range(9)])#################\n",
    "genes_features=genes_features.reshape(9,1).tolist()#######################genes_features[i][0]=label\n",
    "\n",
    "inn_model.cuda()\n",
    "\n",
    "###[3, 25.353026075712233, tensor([ 182,  879,  103, 2657, 2489,  914, 2437,  180, 2417, 1402, 2344, 2947,\n",
    "###values=tensor([0.0572, 0.0495, 0.0404, 0.0381, 0.0364, 0.0353, 0.0328, 0.0302, 0.0284,\n",
    "#        0.0257, 0.0180, 0.0164, 0.0155, 0.0150, 0.0135, 0.0127, 0.0121, 0.0120,\n",
    "#        0.0114, 0.0107, 0.0107, 0.0104, 0.0100, 0.0100, 0.0099, 0.0091, 0.0091,\n",
    "#       0.0088, 0.0083, 0.0082, 0.0081, 0.0079, 0.0078, 0.0077, 0.0076, 0.0074,\n",
    "#       0.0073, 0.0072, 0.0072, 0.0071], dtype=torch.float64),\n",
    "###indices=tensor([ 182,  879,  103, 2657, 2489,  914, 2437,  180, 2417, 1402, 2344, 2947,\n",
    "#        2546, 1114,  796, 1111, 2472, 2326, 1274,  932, 2476,  716,  989, 3289,\n",
    "#        1252, 2053,  785, 2429, 3015, 1585,  975, 1150, 1155,  726,  823, 2303,\n",
    "#         699,  349, 1792, 1524])), 25.359040192320702, tensor([2489,  826, 1753, 1792,  601, 2303, 1053,  545, 2559,  624, 3256,  762,\n",
    "#        2666,  182, 1881, 1585,  726, 1367, 2405, 1171, 2947, 2093,   14,  265,\n",
    "#         716,  180, 1467, 2207, 3223,  349,  277, 2141, 2878, 2427, 2326, 1111,\n",
    "#         746, 1402, 2150, 1602]), torch.return_types.topk(\n",
    "#values=tensor([0.0456, 0.0445, 0.0406, 0.0349, 0.0277, 0.0273, 0.0246, 0.0236, 0.0225,\n",
    "#        0.0221, 0.0213, 0.0200, 0.0185, 0.0182, 0.0181, 0.0153, 0.0151, 0.0142,\n",
    "##        0.0141, 0.0140, 0.0138, 0.0123, 0.0121, 0.0115, 0.0109, 0.0108, 0.0107,\n",
    "#        0.0102, 0.0102, 0.0098, 0.0094, 0.0093, 0.0092, 0.0089, 0.0082, 0.0081,\n",
    "#        0.0080, 0.0080, 0.0079, 0.0076], dtype=torch.float64),\n",
    "#indices=tensor([2489,  826, 1753, 1792,  601, 2303, 1053,  545, 2559,  624, 3256,  762,\n",
    "#        2666,  182, 1881, 1585,  726, 1367, 2405, 1171, 2947, 2093,   14,  265,\n",
    "#         716,  180, 1467, 2207, 3223,  349,  277, 2141, 2878, 2427, 2326, 1111,\n",
    "#         746, 1402, 2150, 1602]))]\n",
    "\n",
    "row_mean_tensor=row_mean_tensor.cuda()\n",
    "#print('batch_size:',batch_size)#=20\n",
    "evidence_for_class = []\n",
    "#print(\"target:\",target.shape)\n",
    "#print('target:',target[3])\n",
    "# Overlay with noise \n",
    "# data[0] += 0.25 * data[0].max() * torch.Tensor(np.random.randn(28*28).reshape(1, 28, 28))\n",
    "#model_prediction, true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "contri_k=[]\n",
    "contri_k_names=[]\n",
    "for i in range(9):#10类\n",
    "    # Unfortunately, we had some issue with freeing pytorch memory, therefore \n",
    "    # we need to reevaluate the model separately for every class.\n",
    "        model_prediction, input_relevance_values = inn_model.innvestigate(in_tensor=row_mean_tensor)\n",
    "        #model_prediction, input_relevance_values = inn_model.innvestigate(in_tensor=xm)\n",
    "        evidence_for_class.append(input_relevance_values)\n",
    "        top_k=torch.topk(input_relevance_values,100,largest=True)#################20\n",
    "        #print('top_k:',top_k)\n",
    "    \n",
    "        contri_k.append(top_k)\n",
    "        top_k_indices=top_k.indices.cpu().detach().numpy().tolist()\n",
    "        #print('top_k_indices:',top_k_indices)\n",
    "        # 将二维列表转换为一维列表\n",
    "        flattened_indices = [item for sublist in top_k_indices for item in sublist]\n",
    "\n",
    "        top_k_names=[columns[j] for j in flattened_indices]\n",
    "        #print('top_k_names:',top_k_names)\n",
    "        contri_k_names.append(top_k_names)\n",
    "        #print('contri_k_names:',contri_k_names)\n",
    "        \n",
    "print('begin=============================')\n",
    "l=contri_k_names[4]\n",
    "#l1=contri_k_names[0]\n",
    "s=set(l)\n",
    "inters=[]\n",
    "for i in contri_k_names:\n",
    "    if i!=4:\n",
    "        s1=set(i)\n",
    "        inter=s-s1\n",
    "        print(len(inter))\n",
    "        #s1=set1.intersection(set2)\n",
    "        inters.append(inter)\n",
    "union_set = set().union(*inters)\n",
    "\n",
    "# 如果需要结果为列表形式\n",
    "union_list = list(union_set)\n",
    "print(union_list)\n",
    "print(len(union_list))\n",
    "print('end=================================')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "contri_k_names_np=np.array(contri_k_names)\n",
    "#print(contri_k_names_np.shape)\n",
    "\n",
    "contri_k_names_df=pd.DataFrame(contri_k_names_np)\n",
    "contri_k_names_df.to_csv('results_genes/ConvAttMLP_10layers_add//LRP/contri_100_names_df.csv')\n",
    "#from chartGPT of the following code\n",
    "result = set()\n",
    "for sublist in contri_k_names:\n",
    "    result = result.union(sublist)\n",
    "\n",
    "# 或者使用集合的union方法的更简洁写法，使用集合解析\n",
    "# result = set().union(*list_of_lists)\n",
    "\n",
    "# 或者使用 | 运算符\n",
    "# result = set().union(*list_of_lists)\n",
    "\n",
    "# 将结果转回列表\n",
    "result_list = list(result)\n",
    "\n",
    "#print(result_list)\n",
    "\n",
    "import csv\n",
    "csv_filename = \"results_genes/ConvAttMLP_10layers_add//LRP/genes_all_100_mean.csv\"\n",
    "\n",
    "# 使用CSV模块保存列表为CSV文件\n",
    "with open(csv_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    writer.writerow(result_list)\n",
    "\n",
    "#这将输出所有列表的并集，不包含重复的元素。在这个例子中，结果会是 [1, 2, 3, 4, 5, 6, 7]。你可以根据你的实际需求将这个结果转换为列表或者保留为集合，具体取决于你的应用场景。\n",
    "#抽取训练集和测试集中的数据\n",
    "\n",
    "#生成新选取元素的数据集\n",
    "train_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv'\n",
    "test_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/test_info.csv'\n",
    "\n",
    "train_df=pd.read_csv(train_path,sep=',')\n",
    "train_df1=train_df\n",
    "#print(train_df1.columns)\n",
    "train_df=train_df.iloc[:,1:]\n",
    "train_df1=train_df1.iloc[:,1:-1]\n",
    "\n",
    "columns=result_list\n",
    "columns.append('label')\n",
    "#print(columns)\n",
    "\n",
    "\n",
    "train_df2=train_df[columns]\n",
    "#print(train_df2.shape)\n",
    "train_df2.to_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/ConvAttMLP_10layers_add//LRP/train_val_info_100.csv')\n",
    "\n",
    "test_df=pd.read_csv(test_path,sep=',')\n",
    "\n",
    "test_df=test_df.iloc[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "test_df=test_df[columns]\n",
    "#print(test_df.shape)\n",
    "test_df.to_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/ConvAttMLP_10layers_add//LRP/test_info_100.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35a71ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3300])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros([1, 3300])\n",
    "y=torch.squeeze(x,0)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3f9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
