{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ac83c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAttMLP_res_concat(\n",
       "  (bn0): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=3300, out_features=1100, bias=True)\n",
       "  (bn1): BatchNorm1d(1100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=2200, out_features=100, bias=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
       "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (conv1): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (bn4): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (bn5): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
       "  (pool3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn6): BatchNorm1d(1100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn7): BatchNorm1d(2200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avg_pool): AvgPool1d(kernel_size=(3300,), stride=(3300,), padding=(0,))\n",
       "  (max_pool): MaxPool1d(kernel_size=3300, stride=3300, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Sequential(\n",
       "    (0): Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(2, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import torchmetrics\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "'''\n",
    "def cosine_similarity_matrix(v1, v2):\n",
    "    # 将输入的向量转换为numpy数组\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    \n",
    "    # 计算点积\n",
    "    dot_product = np.sum(v1 * v2)\n",
    "    \n",
    "    # 计算向量的模\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / (norm_v1 * norm_v2)\n",
    "    \n",
    "    return cosine_sim\n",
    "    \n",
    "'''\n",
    "class ConvAttMLP_res_concat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(3300)\n",
    "        self.fc1 = nn.Linear(3300, 1100)\n",
    "        self.bn1= nn.BatchNorm1d(1100)\n",
    "        self.fc2 = nn.Linear(2200, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 8, 5, padding=2)\n",
    "        #self.pool1 = nn.MaxPool1d(3,stride=3)\n",
    "        self.bn4=nn.BatchNorm1d(3300)\n",
    "        self.conv2 = nn.Conv1d(8, 16, 5,padding=2)\n",
    "        self.bn5=nn.BatchNorm1d(3300)\n",
    "        self.pool2 = nn.MaxPool1d(3,stride=3)\n",
    "        self.conv3=nn.Conv1d(16,1,1,stride=1)\n",
    "        self.pool3=nn.MaxPool1d(3,stride=3)\n",
    "        self.bn6=nn.BatchNorm1d(1100)\n",
    "        #self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        #self.fc2 = nn.Linear(120, 84)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "        self.bn7=nn.BatchNorm1d(2200)\n",
    "        \n",
    "        #self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.avg_pool=nn.AvgPool1d(kernel_size=3300)\n",
    "        #self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.max_pool=nn.MaxPool1d(kernel_size=3300)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv1d(16, 16 // 8, 1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16 // 8, 16, 1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        #print('x:',x.shape)\n",
    "        x1= F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        \n",
    "        \n",
    "        x0=torch.unsqueeze(x,1)\n",
    "        #print(\"x0:\",x0.shape)\n",
    "        \n",
    "        \n",
    "        x2=F.relu(self.conv1(x0))\n",
    "        #x2=x2+x0\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=F.relu(self.conv2(x2))\n",
    "        #x2=x2+x0\n",
    "        #print('x2:',x2.shape)\n",
    "        \n",
    "        #print('x2_argpool:',x2.shape)\n",
    "        avg_pool=self.avg_pool(x2)\n",
    "        #print('avg_pool:',avg_pool.shape)\n",
    "        avg_out = self.fc(self.avg_pool(x2))\n",
    "        max_pool=self.max_pool(x2)\n",
    "        #print('max_pool:',max_pool.shape)\n",
    "        max_out = self.fc(self.max_pool(x2))\n",
    "        out = avg_out + max_out\n",
    "        x2=torch.mul(x2,out)\n",
    "        x2=F.relu(self.conv3(x2))\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=x0+x2\n",
    "        \n",
    "        #print('x2:',x2.shape)\n",
    "        #x2=x0+x2####skip connection\n",
    "        #x2=torch.squeeze(x2,1)\n",
    "        #x2=self.bn5(x2)\n",
    "        #x2 = torch.unsqueeze(x2, 1)  # 在第 1 维（通道维）添加维度#####################################\n",
    "        #x2 = self.pool3(x2)  # 现在可以通过 MaxPool1d 了\n",
    "        x2=self.pool3(x2)\n",
    "        #print('x2:',x2.shape)\n",
    "        x2=torch.squeeze(x2,1)\n",
    "        x2=self.bn6(x2)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #x3=torch.concat([x1,x2],dim=1)\n",
    "        #print('x2_final:',x2.shape)\n",
    "        #x3=torch.add(x1,x2)\n",
    "        #att=F.cosine_similarity(x1, x2)\n",
    "        x3=torch.cat([x1,x2],dim=1)\n",
    "        x4=self.bn7(x3)\n",
    "        x4 = F.relu(self.drop(self.bn2(self.fc2(x4))))\n",
    "        return F.softmax(self.bn3(self.fc3(x4)), dim=1) ,x1,x2\n",
    "modela=ConvAttMLP_res_concat()\n",
    "modela.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "badc997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model4)\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "#import config\n",
    "#import evaluate\n",
    "#import data_utils\n",
    "#import Trainer\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "class KZDatasetPredict(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, df_list):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(df_list)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data,label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "   \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,df_list):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        print('data_info:',df_list[-1])\n",
    "        #df=pd.read_csv(csv_path,sep=',')\n",
    "        #df=df.iloc[:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        label=int(df_list[-1])\n",
    "        labels.append(label)\n",
    "        print('labels:',labels)\n",
    "        data=df_list[:-1]\n",
    "        #df_np=np.array(df_list)\n",
    "        #print(rows,cols)\n",
    "        \n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        print('labels.shape:',labels.shape)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        \n",
    "           \n",
    "        \n",
    "        data=np.array(data)#\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        data=torch.from_numpy(data)#\n",
    "            \n",
    "        labels=torch.from_numpy(labels)#\n",
    "        #bi_data=embding_process(nfm_config,data)\n",
    "        #print(\"bi_data.shape:\",bi_data.shape)\n",
    "            \n",
    "            \n",
    "        data_info.append((data,label))\n",
    "        return data_info\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "   \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        df=df.iloc[:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_matrix(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity matrix between two tensors.\n",
    "\n",
    "    Parameters:\n",
    "    tensor1 (torch.Tensor): The first tensor.\n",
    "    tensor2 (torch.Tensor): The second tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A matrix where each element (i, j) is the cosine similarity between tensor1[i] and tensor2[j].\n",
    "    \"\"\"\n",
    "    # 确保输入是张量\n",
    "    tensor1 = torch.tensor(tensor1) if not isinstance(tensor1, torch.Tensor) else tensor1\n",
    "    tensor2 = torch.tensor(tensor2) if not isinstance(tensor2, torch.Tensor) else tensor2\n",
    "    \n",
    "    # 将张量转换为浮点数以避免数值问题\n",
    "    tensor1 = tensor1.float()\n",
    "    tensor2 = tensor2.float()\n",
    "    \n",
    "    # 计算张量的范数\n",
    "    norm1 = torch.norm(tensor1, dim=1, keepdim=True)\n",
    "    norm2 = torch.norm(tensor2, dim=1, keepdim=True)\n",
    "    \n",
    "    # 计算点积\n",
    "    dot_product = torch.mm(tensor1, tensor2.t())\n",
    "    \n",
    "    # 计算余弦相似度矩阵\n",
    "    similarity_matrix = dot_product / (norm1 * norm2.t())\n",
    "    \n",
    "    return similarity_matrix\n",
    "'''\n",
    "# 示例使用\n",
    "tensor1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "tensor2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "similarity_matrix = cosine_similarity_matrix(tensor1, tensor2)\n",
    "print(similarity_matrix)\n",
    "'''        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "       \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        df=df.iloc[:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        \n",
    "        print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "#import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import torchmetrics\n",
    "            \n",
    "\n",
    "from torchmetrics.classification import accuracy\n",
    "\n",
    "def train_epoch(model,train_loader,batch_size,optimizer,loss_func):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "            \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]\n",
    "            \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        y_predict,x1 ,x2= model(x)\n",
    "        att=cosine_similarity_matrix(x1,x2)\n",
    "        #print('-----:',y_predict.shape)\n",
    "        #att.cuda()\n",
    "        #att=F.cosine_similarity(x1, x2)  \n",
    "        #att_loss = att.sum()\n",
    "        #print(\"Sum of all elements in the cosine similarity matrix:\", sum_of_elements.item())\n",
    "        \n",
    "        loss = loss_func(y_predict, labels)+ 1-torch.square(att.sum())\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        #\n",
    "        '''\n",
    "        train_acc_en=torchmetrics.Accuracy(task='multiclass',num_classes=9).cuda()\n",
    "        batch_train_acc=train_acc_en(y_predict,labels_int)\n",
    "        '''\n",
    "        #batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        batch_train_acc = torchmetrics.functional.accuracy(y_predict, labels_int, task='multiclass', num_classes=nfm_config['n_class'])\n",
    "\n",
    "        #batch_train_acc=torchmetrics.classification.Accuracy(y_predict,labels_int)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #train_acc_en=accuracy(y_predict,labels_int)\n",
    "        #batch_train_acc=train_acc_en()\n",
    "        \n",
    "        total_train_accuracy+=batch_train_acc\n",
    "            \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "\n",
    "def val_epoch(model,test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "           \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat,_,_ = model(inputs)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #test_acc_en=torchmetrics.Accuracy(task='multiclass',num_classes=9).cuda()\n",
    "            #batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            batch_test_acc = torchmetrics.functional.accuracy(yhat, targets, task='multiclass', num_classes=nfm_config['n_class'])\n",
    "            total_test_acc+=batch_test_acc\n",
    "            \n",
    "            batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5671595",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':2000,\n",
    "    #'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    #'embed_input_dim':1001,#embed输入维度\n",
    "    #'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 4,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    #'train_data':'dataset/qiuguan/encode/encode_1000/train/train_encode_data_1000_new.csv',\n",
    "    #'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    #'guan_test_data':'dataset/qiuguan/non_code/guan_test/guan_test_data.csv',\n",
    "    #'guan_test_label':'dataset/qiuguan/non_code/guan_test/guan_test_label.csv',\n",
    "    #'test_data':'dataset/qiuguan/encode/encode_1000/test/test_encode_data_1000_new.csv',\n",
    "    #'test_label':'dataset/qiuguan/non_code/test/test_labels.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "216f0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549 3301\n",
      "549 3301\n",
      "-----: torch.Size([4, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4531/301164737.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(x,dtype=torch.float)\n",
      "/tmp/ipykernel_4531/301164737.py:318: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels=torch.tensor(labels,dtype=torch.float)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m epoches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m101\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoches):\n\u001b[0;32m---> 39\u001b[0m     train_loss_total,acc_train\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnfm_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     train_loss_total_list\u001b[38;5;241m.\u001b[39mappend(train_loss_total)\u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_id \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[22], line 331\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, batch_size, optimizer, loss_func)\u001b[0m\n\u001b[1;32m    327\u001b[0m att\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mcosine_similarity(x1, x2)  \n\u001b[1;32m    328\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(y_predict, labels)\u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39msquare(att)\n\u001b[0;32m--> 331\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    334\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/autograd/__init__.py:141\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    138\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    140\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 141\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/my_pytorch_gnn/lib/python3.8/site-packages/torch/autograd/__init__.py:50\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "model=modela\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "for ki in range(K):\n",
    "    trainset = KZDataset(csv_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv',K=K, n_class=nfm_config['n_class'],ki=ki,  typ='train', transform=None, rand=True)\n",
    "    valset = KZDataset(csv_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv', K=K,n_class=nfm_config['n_class'],ki=ki,  typ='val', transform=None, rand=True)\n",
    "    train_loader = data.DataLoader(\n",
    "         dataset=trainset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True)\n",
    "    val_loader = data.DataLoader(\n",
    "         dataset=valset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "    \n",
    "    #model_path='models/ConvAttMLP_res_concat/'\n",
    "    model_path=path='models/ConvAttMLP_res_concat_att_loss/'\n",
    "    #BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    #total = 0\n",
    "    \n",
    "    \n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    num=0\n",
    "   \n",
    "    \n",
    "    epoches=101\n",
    "    for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        train_loss_total,acc_train=train_epoch(model,train_loader,nfm_config['batch_size'],optimizer,loss_func)\n",
    "        train_loss_total_list.append(train_loss_total)#\n",
    "        if epoch_id %20==0:\n",
    "            num=num+1\n",
    "            #path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            #torch.save(model.state_dict(),path)\n",
    "            # 保存模型\n",
    "            \n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pt')##################pt\n",
    "            torch.save(model.state_dict(),path)\n",
    "            #torch.save(model.state_dict(),path)\n",
    "    print(\"the \",ki,\" epoch ends\")\n",
    "    plotLoss(train_loss_total_list,epoches)\n",
    "    train_loss_total_list=[]\n",
    "    acc_test=val_epoch(model,val_loader,nfm_config['batch_size'],optimizer)\n",
    "    print(\"acc_test_each_k:\",acc_test)\n",
    "    test_metrics.append(acc_test)\n",
    "\n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e2377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the two vectors is: 0.9920645329164459\n"
     ]
    }
   ],
   "source": [
    "#####################################test\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_matrix(v1, v2):\n",
    "    # 将输入的向量转换为numpy数组\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    \n",
    "    # 计算点积\n",
    "    dot_product = np.sum(v1 * v2)\n",
    "    \n",
    "    # 计算向量的模\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    cosine_sim = dot_product / (norm_v1 * norm_v2)\n",
    "    \n",
    "    return cosine_sim\n",
    "\n",
    "# 示例向量\n",
    "vector1 = [1, 2, 3]\n",
    "vector2 = [4,5,9]\n",
    "\n",
    "# 计算余弦相似度\n",
    "similarity = cosine_similarity_matrix(vector1, vector2)\n",
    "print(\"The cosine similarity between the two vectors is:\", similarity)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50d237f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9734, 0.9676],\n",
      "        [0.9987, 0.9972]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_matrix(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity matrix between two tensors.\n",
    "\n",
    "    Parameters:\n",
    "    tensor1 (torch.Tensor): The first tensor.\n",
    "    tensor2 (torch.Tensor): The second tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A matrix where each element (i, j) is the cosine similarity between tensor1[i] and tensor2[j].\n",
    "    \"\"\"\n",
    "    # 确保输入是张量\n",
    "    tensor1 = torch.tensor(tensor1) if not isinstance(tensor1, torch.Tensor) else tensor1\n",
    "    tensor2 = torch.tensor(tensor2) if not isinstance(tensor2, torch.Tensor) else tensor2\n",
    "    \n",
    "    # 将张量转换为浮点数以避免数值问题\n",
    "    tensor1 = tensor1.float()\n",
    "    tensor2 = tensor2.float()\n",
    "    \n",
    "    # 计算张量的范数\n",
    "    norm1 = torch.norm(tensor1, dim=1, keepdim=True)\n",
    "    norm2 = torch.norm(tensor2, dim=1, keepdim=True)\n",
    "    \n",
    "    # 计算点积\n",
    "    dot_product = torch.mm(tensor1, tensor2.t())\n",
    "    \n",
    "    # 计算余弦相似度矩阵\n",
    "    similarity_matrix = dot_product / (norm1 * norm2.t())\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# 示例使用\n",
    "tensor1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "tensor2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "similarity_matrix = cosine_similarity_matrix(tensor1, tensor2)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c226c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
