{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8282340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn0.weight\n",
      "bn0.bias\n",
      "bn0.running_mean\n",
      "bn0.running_var\n",
      "bn0.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "bn2.running_mean\n",
      "bn2.running_var\n",
      "bn2.num_batches_tracked\n",
      "fc3.weight\n",
      "fc3.bias\n",
      "bn3.weight\n",
      "bn3.bias\n",
      "bn3.running_mean\n",
      "bn3.running_var\n",
      "bn3.num_batches_tracked\n",
      "MLP(\n",
      "  (bn0): BatchNorm1d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=244, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    }
   ],
   "source": [
    "#############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "input_num=244\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    \n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "    \n",
    "    \n",
    "\n",
    "#model1 = MLP().cuda()\n",
    "#print(model1)\n",
    "\n",
    "class MLP_P(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3)    \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3) \n",
    "        \n",
    "        #self.model1=MLP1().cuda() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        y1=self.bn0(x)\n",
    "        y1 = F.relu(self.drop(self.bn1(self.fc1(y1))))\n",
    "        y1= F.relu(self.drop(self.bn2(self.fc2(y1))))\n",
    "        return F.softmax(self.bn3(self.fc3(y1)), dim=1)\n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "model = MLP().cuda()\n",
    "\n",
    "for i in model.state_dict():\n",
    "    print(i)\n",
    "print(model)\n",
    "mlp_paras=list(model.named_parameters())\n",
    "#print(mlp_paras)\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        \n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        \n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            \n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            \n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "       \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "def standN(x):\n",
    "    row,col=x.shape\n",
    "    max_x=torch.max(x,1)\n",
    "    min_x=torch.min(x,1)\n",
    "    #print(max_x)\n",
    "    #print(min_x)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-min_x.values[i])/max_x.values[i]\n",
    "    return x\n",
    "\n",
    "def standNorm(x):\n",
    "    row,col=x.shape\n",
    "    mean=torch.mean(x,1)\n",
    "    std=torch.std(x,1)\n",
    "    #print(mean)\n",
    "    #print(std)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-mean[i])/std[i]\n",
    "    return x\n",
    "\n",
    "def augment_epoch(kii,train_loader,batch_size):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    loss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    #BL=nn.BatchNorm1d(input_num)\n",
    "    #BL=BL.cuda()\n",
    "    y=[[0]*input_num]*batch_size\n",
    "    y=torch.tensor(y,dtype=torch.float)\n",
    "    y=y.cuda()\n",
    "    \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        ll=labels.view(batch_size,-1)\n",
    "        #ll.dtype=torch.int\n",
    "        #print('ll:',ll)\n",
    "        if kii>=0 : \n",
    "            \n",
    "            inn_model.evaluate(in_tensor=x)\n",
    "        \n",
    "            model_prediction, only_max_score,org_shape = inn_model.innvestigatex()\n",
    "            model_prediction.cuda()\n",
    "            #print('only_max_score:',only_max_score)\n",
    "            #only_max_score1=only_max_score.detach().clone()\n",
    "            #print(labels.shape,labels)   \n",
    "        \n",
    "            #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "            #print(model_prediction.shape, model_prediction)\n",
    "            #model_prediction.cuda()\n",
    "            #print('input_relevance_values:')\n",
    "            rel_for_class_list=torch.argmax(model_prediction,1)\n",
    "            #print(rel_for_class_list)\n",
    "            para_list=[]\n",
    "            \n",
    "            '''  \n",
    "            rand=torch.linspace(0, 1, steps=input_num)\n",
    "            rand=rand.detach().numpy().tolist()\n",
    "            \n",
    "            para_list=[round(i,4) for i in rand]\n",
    "            '''\n",
    "            \n",
    "            #print(para_list)\n",
    "            \n",
    "            \n",
    "            for i in range(batch_size):\n",
    "            \n",
    "                max_pre=torch.argmax(model_prediction[i])\n",
    "                if max_pre!=labels[i]:\n",
    "                    #print('only_max_score1:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                    #rel_for_class=labels[i]\n",
    "                    #para=random.gauss(0.6,0.4)#########0.6，0.4：98  96\n",
    "                    \n",
    "                    #para=random.gauss(0.6,0.7)############0.6  0.7  96  95\n",
    "                    \n",
    "                    #para=random.gauss(0.7,0.4)###############\n",
    "                    #para=random.gauss(0.7,0.3)#############9110  96  98，0.2，0.1\n",
    "                    #para=random.gauss(0.6,0.5)########96  98  0.1  0.1  9110\n",
    "                    #para=random.gauss(0.6,0.2)#######0.1 0.1  96  96\n",
    "                    para=random.gauss(0.6,0.4)############7510pkl   96  96  0.6  0.4  0.1  0.1\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "                    #print('device:',input_relevance_values.device)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                    #print('pre_target:',pre_target[i])\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                else:\n",
    "                    #print('only_max_score2:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                    para=random.gauss(0.1,0.2)\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "            \n",
    "            \n",
    "            \n",
    "            input_relevance_values,layers=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "            input_len=len(input_relevance_values)\n",
    "            \n",
    "            ee=[[0]*input_num]*batch_size\n",
    "            ee=torch.tensor(ee,dtype=torch.float)\n",
    "        \n",
    "            ee=ee.cuda()\n",
    "            input_relevance_values[-1]=input_relevance_values[-1].exp()############\n",
    "            \n",
    "            \n",
    "            #sum_input_relevance_values=torch.sum(input_relevance_values,dim=0)\n",
    "            #input_relevance_values=F.softmax(input_relevance_values/sum_input_relevance_values,dim=1)\n",
    "            input_relevance_values[-1]=F.softmax(input_relevance_values[-1],dim=1)################\n",
    "        \n",
    "            '''\n",
    "            for i in range(batch_size):\n",
    "                        max_pre=torch.argmax(model_prediction[i])######\n",
    "                        if max_pre!=labels[i]: #and i==min_predict_mis:###\n",
    "                            \n",
    "                                \n",
    "            \n",
    "                                ee[i]=torch.mul(x[i],input_relevance_values[-1][i])##############注意力33333333333333333weight\n",
    "                                #dd=torch.mul(bias,input_relevance_values[-1][i])############bias\n",
    "                                #print('cc:',i,cc)\n",
    "                                #ee[i]=torch.mul(drop,input_relevance_values[i])\n",
    "                                ################\n",
    "            '''                    \n",
    "            ee=torch.mul(x,input_relevance_values[-1])\n",
    "            \n",
    "            \n",
    "            x1=torch.subtract(x,ee)\n",
    "            x2=torch.add(x,ee)\n",
    "            #print(ll.shape)\n",
    "            #print(x.shape)\n",
    "            #x1=torch.cat((x1,ll),dim=1)\n",
    "            #x2=torch.cat((x2,ll),dim=1)\n",
    "            \n",
    "            l1=ll\n",
    "            l2=ll\n",
    "            if batch_idx==0:\n",
    "                \n",
    "                y=torch.cat((x1,x2),dim=0)\n",
    "                l=torch.cat((l1,l2),dim=0)\n",
    "            y=torch.cat((y,x1),dim=0)\n",
    "            y=torch.cat((y,x2),dim=0)\n",
    "            \n",
    "            l=torch.cat((l,l1),dim=0)\n",
    "            l=torch.cat((l,l2),dim=0)\n",
    "    \n",
    "    \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "              \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    '''  \n",
    "        #print('x_new:',x)\n",
    "        #print('new_x:',x)\n",
    "        optimizer.zero_grad()\n",
    "        y_predict=model(x)\n",
    "        #y_predict=model(x,labels)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss1 = loss_func(y_predict, labels)\n",
    "        #print('loss1:',loss1)\n",
    "        #loss2=loss_func(y_predict1,labels)\n",
    "        #loss2=u*(1/loss_op)\n",
    "        #print('input_relevance_values:',input_relevance_values.shape)\n",
    "        #print('loss_score:',loss_score.shape)\n",
    "        #cc=1.0/torch.abs(torch.sum(cc))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #loss2=u*cc\n",
    "        #print('input_relevance_values:',input_relevance_values)\n",
    "        \n",
    "        #print('loss1:',loss1)\n",
    "        #print('loss2:',loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        #loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss2_list.append(u*loss2)   \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "    #plotLoss(loss2_list,batch_idx+1)   #################################     \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "    \n",
    "    ''' \n",
    "    return y,l\n",
    "\n",
    "def val_epoch(test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "        print(\"val\")\n",
    "            \n",
    "            \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)     \n",
    "           \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()\n",
    "        targets=torch.max(targets,1)[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #yhat = model1(inputs,targets)\n",
    "        yhat=model(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "        total_test_acc+=batch_test_acc\n",
    "            \n",
    "        batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d992c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=244, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:440: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:442: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1          2         3         4         5         6    \\\n",
      "0     4.712716  4.719800   7.783406  6.887787  5.105534  6.448301  8.770288   \n",
      "1     4.872344  4.655252   5.551160  6.862397  5.264141  7.233065  8.792705   \n",
      "2     5.121512  4.722118   5.009521  7.260148  4.498782  9.396037  8.355294   \n",
      "3     5.203842  5.147714   9.359057  8.069266  5.415675  7.978174  8.908166   \n",
      "4     5.093595  4.635318   8.815096  6.958769  5.042191  6.857650  8.817730   \n",
      "...        ...       ...        ...       ...       ...       ...       ...   \n",
      "1115  5.549198  4.963156  11.240979  7.623978  5.662553  7.785930  8.764916   \n",
      "1116  5.767074  5.232141  10.508600  6.777960  5.998055  7.694820  8.074890   \n",
      "1117  5.450021  5.427273   9.696732  8.001414  5.380512  6.583365  7.515232   \n",
      "1118  5.315147  5.105342  10.904235  6.793604  5.469623  6.246966  9.170125   \n",
      "1119  6.099760  5.341326  14.266402  7.168535  5.111259  7.491924  8.306478   \n",
      "\n",
      "            7         8         9    ...       235       236        237  \\\n",
      "0      9.620617  6.672391  5.529776  ...  7.850493  4.708017   8.631679   \n",
      "1      9.452042  6.618914  5.287418  ...  7.759112  4.624828   9.004896   \n",
      "2      9.285069  6.794751  4.961425  ...  6.836439  5.621385   8.202138   \n",
      "3      9.414321  6.677217  5.306725  ...  7.136260  5.053957   8.695605   \n",
      "4      9.614713  6.623538  5.651084  ...  7.218055  4.960739   8.371154   \n",
      "...         ...       ...       ...  ...       ...       ...        ...   \n",
      "1115   9.291302  7.084355  5.296731  ...  7.484451  5.237230  10.056620   \n",
      "1116   8.955356  6.876841  5.508345  ...  7.507265  5.045501   7.191704   \n",
      "1117  10.309402  6.331498  5.349448  ...  7.052671  5.264940   9.278662   \n",
      "1118   9.764207  7.262960  6.079003  ...  7.983595  5.136976   8.326666   \n",
      "1119   9.275846  7.428974  5.812338  ...  8.099184  5.344530   7.799361   \n",
      "\n",
      "            238       239       240       241       242       243  0    \n",
      "0      9.686431  4.279988  6.922071  5.914382  9.196198  7.493042    2  \n",
      "1      9.973739  4.592134  8.120454  6.031137  8.692977  7.888149    4  \n",
      "2      9.225848  4.351894  8.357266  5.442379  9.124334  7.013107    1  \n",
      "3      9.352386  4.383204  7.521249  5.799089  8.877832  7.414980    8  \n",
      "4      9.776899  4.607983  7.161503  6.140569  9.266461  7.308853    8  \n",
      "...         ...       ...       ...       ...       ...       ...  ...  \n",
      "1115   9.869749  4.968171  9.087183  5.862158  9.448779  7.600974    1  \n",
      "1116  10.109375  5.414255  6.188144  5.992501  8.894320  7.794433    4  \n",
      "1117  10.594418  5.276006  7.757452  6.015215  9.875643  8.067211    8  \n",
      "1118  10.286660  4.891357  6.650464  6.779995  9.071657  7.580391    6  \n",
      "1119  10.477098  4.834022  6.699968  5.794596  9.004477  7.838853    5  \n",
      "\n",
      "[1120 rows x 245 columns]\n"
     ]
    }
   ],
   "source": [
    "#_,model=MLPA().cuda\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/origin_800/LRP/non_encode/80/without_attention/MLP1010.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "#print(model.state_dict())\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(mlp_params)\n",
    "model=mlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "\n",
    "testset= KZDatasetTest(csv_path='dataset/qiuguan/origin_800/LRP/80/selected_train_val_info.csv')\n",
    "   \n",
    "test_loader = data.DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "\n",
    "#model_path='dataset/qiuguan/origin_800/LRP/non_encode/200/attention0.01/'\n",
    "#BATCH_SIZE=batch_size\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "#total = 0\n",
    "    \n",
    "    \n",
    "#loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "num=0\n",
    "   \n",
    "model_named_parameters=[j for j in model.named_parameters()]\n",
    "#print('model_parameters:',model_named_parameters)\n",
    "#print('model.state.dict:',model.state_dict())\n",
    "epoches=1\n",
    "for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        y,l=augment_epoch(0,test_loader,nfm_config['batch_size'])\n",
    "        #train_loss_total_list.append(train_loss_total)#\n",
    "        \n",
    "        '''  \n",
    "        if epoch_id %10==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "            \n",
    "         '''  \n",
    "y=y.detach().cpu().numpy()\n",
    "y=pd.DataFrame(y)\n",
    "\n",
    "l=l.detach().cpu().numpy()\n",
    "l=pd.DataFrame(l)\n",
    "#print(y)\n",
    "#print(l)\n",
    "\n",
    "\n",
    "aug=pd.concat((y,l),axis=1)\n",
    "aug.to_csv('dataset/qiuguan/origin_800/LRP/80/aug_train_val_info.csv')\n",
    "print(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bf7f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            TYR        PPM1E                REN               ABCC3  \\\n",
      "0        4.71272       4.7198            7.78341             6.88779   \n",
      "1        4.87234      4.65525            5.55116              6.8624   \n",
      "2        5.12151      4.72212            5.00952             7.26015   \n",
      "3        5.20384      5.14771            9.35906             8.06927   \n",
      "4         5.0936      4.63532             8.8151             6.95877   \n",
      "..           ...          ...                ...                 ...   \n",
      "541  5.743529064  5.210875345        10.46588395  6.7502614670000005   \n",
      "542  5.427847751  5.405209976  9.657287377000001         7.968845876   \n",
      "543  5.293333995  5.084620506        10.85997562   6.765459452999999   \n",
      "544   6.07443638  5.319655345        14.20590899         7.139406749   \n",
      "545   5.19511412  4.927471871         6.83466967  7.9497790739999985   \n",
      "\n",
      "0           LRP5           IGLV1-40       DNAJB1              ABCD3  \\\n",
      "0        5.10553             6.4483      8.77029            9.62062   \n",
      "1        5.26414            7.23307       8.7927            9.45204   \n",
      "2        4.49878            9.39604      8.35529            9.28507   \n",
      "3        5.41568            7.97817      8.90817            9.41432   \n",
      "4        5.04219            6.85765      8.81773            9.61471   \n",
      "..           ...                ...          ...                ...   \n",
      "541  5.973083531  7.663390122999999  8.041910695  8.918939803999999   \n",
      "542  5.358618036         6.55656512   7.48444839        10.26745748   \n",
      "543   5.44740957        6.220947165  9.132807376  9.724481127999999   \n",
      "544  5.089893268  7.461380137000001  8.272700125        9.238210411   \n",
      "545  5.215868235  6.744153932000001  8.668916388         8.96872861   \n",
      "\n",
      "0          FKBP4        NUBPL  ...             ZNF593               AADAC  \\\n",
      "0        6.67239      5.52978  ...            7.85049             4.70802   \n",
      "1        6.61891      5.28742  ...            7.75911             4.62483   \n",
      "2        6.79475      4.96143  ...            6.83644             5.62138   \n",
      "3        6.67722      5.30673  ...            7.13626             5.05396   \n",
      "4        6.62354      5.65108  ...            7.21805             4.96074   \n",
      "..           ...          ...  ...                ...                 ...   \n",
      "541  6.848810165  5.485931233  ...         7.47670395         5.024974264   \n",
      "542  6.305334465  5.327675867  ...  7.023943157000001         5.243536465   \n",
      "543  7.233185653  6.054224798  ...        7.951179894         5.116125795   \n",
      "544  7.398263494   5.78875476  ...        8.066300433  5.3228251680000005   \n",
      "545  7.009632536  5.524441201  ...         7.38484989         5.092608074   \n",
      "\n",
      "0                FBLN5      ATP5IF1        MUC16     SERPINF1  \\\n",
      "0              8.63168      9.68643      4.27999      6.92207   \n",
      "1               9.0049      9.97374      4.59213      8.12045   \n",
      "2              8.20214      9.22585      4.35189      8.35727   \n",
      "3              8.69561      9.35239       4.3832      7.52125   \n",
      "4              8.37115       9.7769      4.60798       7.1615   \n",
      "..                 ...          ...          ...          ...   \n",
      "541        7.162388038  10.06828642  5.392151009   6.16271403   \n",
      "542        9.240928392  10.55134956  5.254541275   7.72590708   \n",
      "543        8.292869487  10.24490888  4.871486101  6.623344071   \n",
      "544  7.767682237000001  10.43459007  4.814384327  6.672702295   \n",
      "545        7.895972466  9.684586981  4.789443717  7.532901706   \n",
      "\n",
      "0               MRPL44              CAMLG              ABCB7 label  \n",
      "0              5.91438             9.1962            7.49304     2  \n",
      "1              6.03114            8.69298            7.88815     4  \n",
      "2              5.44238            9.12433            7.01311     1  \n",
      "3              5.79909            8.87783            7.41498     8  \n",
      "4              6.14057            9.26646            7.30885     8  \n",
      "..                 ...                ...                ...   ...  \n",
      "541  5.968105592000001        8.857877495        7.762717611     4  \n",
      "542  5.990752387000001  9.835309092000001        8.034228317     8  \n",
      "543        6.751462625  9.034547868999999        7.549562876     6  \n",
      "544  5.770955197999999        8.967589079  7.806921347999999     5  \n",
      "545        5.715177177        9.273350113        7.185852079     7  \n",
      "\n",
      "[1665 rows x 245 columns]\n"
     ]
    }
   ],
   "source": [
    "x=pd.read_csv('dataset/qiuguan/origin_800/LRP/80/selected_train_val_info.csv',sep=',')\n",
    "#print(x)\n",
    "\n",
    "#x=x.iloc[:,1:]\n",
    "\n",
    "#print(x)\n",
    "\n",
    "column=x.iloc[0,1:]#列名\n",
    "#print(column)\n",
    "\n",
    "x_new=x.iloc[1:,1:]\n",
    "x_new.columns=column\n",
    "#print(x_new)\n",
    "aug.columns=column\n",
    "#print(aug)\n",
    "\n",
    "all=pd.concat((aug,x_new),axis=0)\n",
    "print(all)\n",
    "\n",
    "all.to_csv('dataset/qiuguan/origin_800/LRP/80/all_train_val_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b34c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
