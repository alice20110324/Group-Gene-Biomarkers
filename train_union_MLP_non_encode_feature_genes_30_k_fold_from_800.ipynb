{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fab9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(3301, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3301, out_features=800, bias=True)\n",
      "  (bn1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=800, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':2000,\n",
    "    #'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    #'embed_input_dim':1001,#embed输入维度\n",
    "    #'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,8],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(3301)\n",
    "        self.fc1 = nn.Linear(3301, 800)\n",
    "        self.bn1= nn.BatchNorm1d(800)\n",
    "        self.fc2 = nn.Linear(800, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model = MLP().cuda()\n",
    "print(model)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "#import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "   \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        \n",
    "        df=df.iloc[1:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff87b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(3301, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3301, out_features=800, bias=True)\n",
      "  (bn1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=800, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "relevance\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "合格： tensor(8)\n",
      "合格： tensor(3)\n",
      "合格： tensor(4)\n",
      "合格： tensor(0)\n",
      "合格： tensor(2)\n",
      "合格： tensor(6)\n",
      "合格： tensor(7)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(0)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(4)\n",
      "合格： tensor(1)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(7)\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(6)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(2)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(3)\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "合格： tensor(5)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(2)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(0)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(1)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(3)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(1)\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(1)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(2)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(6)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(5)\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "qiu_____genes_features.shape: [[0, 25.36085344884185, tensor([ 799, 2504, 1200, 1112, 2477,  385, 1054,  103, 1468,   17, 2680,  510,\n",
      "        1870, 1882, 3290, 3042, 1849,  535, 2961,  784, 3086,  223,  976, 2439,\n",
      "         716, 1156, 1423, 2067, 2612, 1653])], [1, 25.357270064490677, tensor([ 799, 2667,  673, 3289, 2067, 1112, 2896, 1125, 3121,  103, 2345, 2504,\n",
      "        3008, 2313, 2750, 1210, 1897,  976,  535, 2430, 1793,  277, 2149, 2428,\n",
      "        2627, 1177,  572, 2412,  735, 3061])], [2, 25.36108076837147, tensor([ 799, 2142, 1947, 2473,  699, 3026,  707, 2718,  880,  370, 2497, 2296,\n",
      "        1172, 1754,  385, 2011,  522, 1855, 2667, 1177, 2438, 1525,  303, 2680,\n",
      "         386,  710,  827, 2077,  574, 1151])], [3, 25.358700453747957, tensor([3121,   17,  799, 3257,  673, 1185, 2412, 2862, 2560, 1125, 2413,  223,\n",
      "        2011,  522,  880,  572, 2067, 1782, 2140, 1947, 1793,  827,  716,  710,\n",
      "        1489, 2077, 2838, 1897,  699, 2680])], [4, 25.36105967238061, tensor([ 726,  699, 1782, 1200, 1112,   89,  522,  223,  915,  103,  710,  335,\n",
      "        1882,  277,  510, 1368, 2560, 1378, 1115, 1151, 2067, 1640, 1947, 2345,\n",
      "        2428,  799,  976, 1525, 1172, 2454])], [5, 25.360425794764495, tensor([ 103,  710, 1210, 2011,  180,  991, 2412, 2862, 1793, 2439, 2560, 1200,\n",
      "        3290, 2149, 2413, 2788,  976,  990,  574,  699, 2658,  915,  716, 1782,\n",
      "        1855,  165, 1809, 3008, 2948,  510])], [6, 25.36116586742761, tensor([ 799, 2077, 1054, 1779,  233, 2439,   89,  673,  335, 1115,  784,  385,\n",
      "        3016, 1653, 1200,  487,  684, 2477,  180, 1391, 2603, 3290,  950,  933,\n",
      "         961, 2313, 1153,  535, 2406,  713])], [7, 25.3606530276656, tensor([ 799, 1793, 2011, 1870, 2477,  735, 1210, 2313,  726,  180,   17, 1423,\n",
      "        1604,  990,  277, 1882, 3289, 2439, 1252,  673,  510, 2490,   89,  784,\n",
      "        1115,  827, 1403, 2718, 2149, 2430])], [8, 25.36117121587863, tensor([ 799,  103, 1210,   17, 2410,  385, 2560,  124,  917, 2477,  535,  827,\n",
      "         990,  417, 3016, 1185,  976, 1368,  522,  165, 2412, 2490, 3239, 1450,\n",
      "         223, 3121, 1394, 1054,   89, 2419])]]\n"
     ]
    }
   ],
   "source": [
    "#######找特征基因#############从3301中找200个基因\n",
    "#########################################################本次测试的目的是看200个基因的分类效果\n",
    "##########测试步骤：从3301个基因中提取350个\n",
    "############用200个构建新的分类模型\n",
    "#################特征基因\n",
    "######################为小球，根据上边的测试的基因个数，350最大\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "\n",
    "\n",
    "\n",
    "##############数据准备\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/model_new_K_fold_RandomTree/MLP_non_encode/MLP610.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=mlp\n",
    "model=net###########\n",
    "'''\n",
    "testset = KZDatasetTest(csv_path='../NFM-pyorch-master/dataset/qiuguan/orign/')\n",
    "   \n",
    "test_loader = DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "'''\n",
    "\n",
    "testset_xiaoqiu  = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoqiu/test_info.csv')#样本收集特征数据集，和测试数据集不同，这里边可能还包含训练集\n",
    "   \n",
    "test_loader_xiaoqiu = DataLoader(\n",
    "         dataset=testset_xiaoqiu,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True\n",
    "        \n",
    "     )\n",
    "\n",
    "testset_xiaoguan  = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/test_info.csv')\n",
    "   \n",
    "test_loader_xiaoguan = DataLoader(\n",
    "         dataset=testset_xiaoguan,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True\n",
    "        \n",
    "     )\n",
    "\n",
    "################小球\n",
    "\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "genes_features=np.array([i for i  in range(9)])\n",
    "genes_features=genes_features.reshape(9,1).tolist()\n",
    "model.double()\n",
    "for data, target in test_loader_xiaoqiu:############小球\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    #targets=torch.max(targets,1)[1]###################\n",
    "    #print('data:',data.shape)\n",
    "    batch_size = int(data.size()[0])\n",
    "    #print('batch_size:',batch_size)#=20\n",
    "    evidence_for_class = []\n",
    "    #print(\"target:\",target.shape)\n",
    "    #print('target:',target[3])\n",
    "    # Overlay with noise \n",
    "    # data[0] += 0.25 * data[0].max() * torch.Tensor(np.random.randn(28*28).reshape(1, 28, 28))\n",
    "    #model_prediction, true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "\n",
    "    for i in range(9):#10类\n",
    "    # Unfortunately, we had some issue with freeing pytorch memory, therefore \n",
    "    # we need to reevaluate the model separately for every class.\n",
    "        model_prediction, input_relevance_values = inn_model.innvestigate2(in_tensor=data, rel_for_class=i,target=target)\n",
    "        evidence_for_class.append(input_relevance_values)\n",
    "    #print('input_relevance_values:',input_relevance_values.shape)\n",
    "    #print('evidence_for_class:',len(evidence_for_class))\n",
    "    evidence_for_class = np.array([elt.numpy() for elt in evidence_for_class])\n",
    "    #print('evidence_for_class:',evidence_for_class.shape)#[10,20,784]\n",
    "    for idx, example in enumerate(data):#batch 中的每一个样本\n",
    "        #print('example:',example.shape)\n",
    "        prediction = np.argmax(model_prediction.cpu().detach(), axis=1)#\n",
    "        #print('prediction[idx]:',prediction[idx])\n",
    "        #print(evidence_for_class[prediction[idx]][idx])\n",
    "        #fig, axes = plt.subplots(3, 5)\n",
    "        '''\n",
    "        fig.suptitle(\"Prediction of model: \" + str(prediction[idx]) + \"({0:.2f})\".format(\n",
    "            100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())))\n",
    "        '''\n",
    "        prediction_value=prediction[idx]\n",
    "        p_x=model_prediction[idx][model_prediction[idx].argmax()].exp()\n",
    "        p_sum=model_prediction[idx].exp().sum()\n",
    "        prediction_score=100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())\n",
    "        #print('prediction_value:',prediction_value)\n",
    "        #print('prediction_score:',prediction_score)\n",
    "        #print('分子:',p_x)\n",
    "        #print('分母：',p_sum)\n",
    "        #uu=pr\n",
    "        #print(\"torch.argmax:\",torch.argmax(target[idx]))\n",
    "        if len(genes_features[prediction_value])==1:#有值，但还没有添加预测分数和特征值，只有标签#prediction_value代表第几种疾病\n",
    "            if prediction_value!=torch.argmax(target[idx]).cpu().detach()  :\n",
    "                print('不合格****************:',prediction_value)\n",
    "            if prediction_value==torch.argmax(target[idx]).cpu().detach()  :#预测正确\n",
    "                genes_features[prediction_value].append(prediction_score)\n",
    "                print('合格：',prediction_value)\n",
    "                relevance_score_for_every_pixel=evidence_for_class[prediction[idx]][idx]\n",
    "                #print('relevance_score_for_every_pixel.shape:',relevance_score_for_every_pixel.tolist())\n",
    "                relevance_score_for_every_pixel=torch.from_numpy(relevance_score_for_every_pixel)\n",
    "                index=torch.topk(relevance_score_for_every_pixel,30,largest=True)#基因个数50#####150\n",
    "                #print('pixel_sorted:',index)\n",
    "                genes_features[prediction_value].append(index.indices)#添加前50基因特征\n",
    "        \n",
    "        else: \n",
    "            if genes_features[prediction_value][1]<prediction_score:#如果值比已有值大，说明预测更准确\n",
    "                if prediction_value==torch.argmax(target[idx]).cpu().detach():\n",
    "                    print('&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&:',prediction_value)\n",
    "                    genes_features[prediction_value].pop(2)#先删除特征值\n",
    "                    genes_features[prediction_value].pop(1)#先删除预测分数\n",
    "                \n",
    "                    genes_features[prediction_value].append(prediction_score)\n",
    "        \n",
    "                    relevance_score_for_every_pixel=evidence_for_class[prediction[idx]][idx]\n",
    "                    #print('relevance_score_for_every_pixel.shape:',relevance_score_for_every_pixel.tolist())\n",
    "                    relevance_score_for_every_pixel=torch.from_numpy(relevance_score_for_every_pixel)\n",
    "                    index=torch.topk(relevance_score_for_every_pixel,30,largest=True)#基因个数50  200\n",
    "                    #print('pixel_sorted:',index)\n",
    "                    genes_features[prediction_value].append(index.indices)\n",
    "print('qiu_____genes_features.shape:',genes_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5c987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 30])\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    \n",
    "    qiu_genes_tensor=genes_features[i][2]\n",
    "    if i==0:\n",
    "        mm=qiu_genes_tensor\n",
    "    else:\n",
    "        mm=torch.vstack((mm,qiu_genes_tensor))\n",
    "print(mm.shape)\n",
    "qiu_genes_numpy=mm.detach().numpy()\n",
    "qiu_genes_df=pd.DataFrame(qiu_genes_numpy)\n",
    "qiu_genes_df.to_csv('dataset/qiuguan/origin_800/LRP/30/qiu_genes_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbf188a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(1, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(6, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(8)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(5)\n",
      "&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&: tensor(6)\n",
      "guan______________genes_features.shape: [[0, 25.36085344884185, tensor([ 799, 2504, 1200, 1112, 2477,  385, 1054,  103, 1468,   17, 2680,  510,\n",
      "        1870, 1882, 3290, 3042, 1849,  535, 2961,  784, 3086,  223,  976, 2439,\n",
      "         716, 1156, 1423, 2067, 2612, 1653]), 25.356753523164883, tensor([ 799, 2011,   89, 2667,  827, 1468, 3008, 2627, 1112, 1054,  223, 2345,\n",
      "        2680,  572, 1117,  510, 1032, 1669, 1793, 1210, 2054,  386,  716, 1200,\n",
      "        3016, 2430,  534,  385, 2439, 2313])], [1, 25.357270064490677, tensor([ 799, 2667,  673, 3289, 2067, 1112, 2896, 1125, 3121,  103, 2345, 2504,\n",
      "        3008, 2313, 2750, 1210, 1897,  976,  535, 2430, 1793,  277, 2149, 2428,\n",
      "        2627, 1177,  572, 2412,  735, 3061]), 25.36092397723709, tensor([ 799, 2788,  103, 1403, 2504,  535,  827,  880, 3016,  726,  460,  710,\n",
      "        2142, 2948,  277,  180, 1870, 2477,  716,  522,  198,   89, 2680, 1177,\n",
      "        2562, 3289,  510,  735, 1115,  917])], [2, 25.36108076837147, tensor([ 799, 2142, 1947, 2473,  699, 3026,  707, 2718,  880,  370, 2497, 2296,\n",
      "        1172, 1754,  385, 2011,  522, 1855, 2667, 1177, 2438, 1525,  303, 2680,\n",
      "         386,  710,  827, 2077,  574, 1151]), 25.36109090987163, tensor([ 799, 2142, 2667,  990,  103, 2218,   17, 1334,  710,  961,  522, 2208,\n",
      "        2473,  277, 2410, 1782, 1210,  385, 1156, 2490, 2948, 2313,  510, 1230,\n",
      "        1470, 2054, 1586, 1403,  145,  735])], [3, 25.358700453747957, tensor([3121,   17,  799, 3257,  673, 1185, 2412, 2862, 2560, 1125, 2413,  223,\n",
      "        2011,  522,  880,  572, 2067, 1782, 2140, 1947, 1793,  827,  716,  710,\n",
      "        1489, 2077, 2838, 1897,  699, 2680]), 25.360843106015203, tensor([  17, 2560, 3016, 2409, 2439, 1252,  726, 1947,  716, 3026, 2412, 2011,\n",
      "        2077, 1882,  545, 2349,  556, 1200, 1798, 1754, 1640,  277,  385, 1391,\n",
      "        2627, 2304, 1855,  699, 2149, 2948])], [4, 25.36105967238061, tensor([ 726,  699, 1782, 1200, 1112,   89,  522,  223,  915,  103,  710,  335,\n",
      "        1882,  277,  510, 1368, 2560, 1378, 1115, 1151, 2067, 1640, 1947, 2345,\n",
      "        2428,  799,  976, 1525, 1172, 2454]), 25.361166878721715, tensor([ 180, 1210, 1368,  385,  386,  535, 2627,  277,  522, 2750,   17, 2067,\n",
      "        2680, 2438, 1793,  915, 1525,  665, 2345,  716,  673,  510, 2497, 1403,\n",
      "         735, 1391,  726, 1640,  990,  799])], [5, 25.360425794764495, tensor([ 103,  710, 1210, 2011,  180,  991, 2412, 2862, 1793, 2439, 2560, 1200,\n",
      "        3290, 2149, 2413, 2788,  976,  990,  574,  699, 2658,  915,  716, 1782,\n",
      "        1855,  165, 1809, 3008, 2948,  510]), 25.360814563519764, tensor([1210,  165,  522, 1115,  799,  990,  673, 2011,  699, 2412,  535, 2410,\n",
      "         915, 1947,  707, 1798, 1525,  385, 1787, 3289, 2603, 3008, 2627,  180,\n",
      "        1653,  880,  370, 2304, 2067,  103])], [6, 25.36116586742761, tensor([ 799, 2077, 1054, 1779,  233, 2439,   89,  673,  335, 1115,  784,  385,\n",
      "        3016, 1653, 1200,  487,  684, 2477,  180, 1391, 2603, 3290,  950,  933,\n",
      "         961, 2313, 1153,  535, 2406,  713]), 25.360922722079522, tensor([ 799,  103,  990, 2412, 2862,  827, 1779, 1368, 1391, 1200, 1793,  976,\n",
      "         535, 1653, 1604, 2418, 2428, 2149, 2560,  180, 2438,  673,  710, 2477,\n",
      "         880, 1272, 2788, 1185,  277, 1450])], [7, 25.3606530276656, tensor([ 799, 1793, 2011, 1870, 2477,  735, 1210, 2313,  726,  180,   17, 1423,\n",
      "        1604,  990,  277, 1882, 3289, 2439, 1252,  673,  510, 2490,   89,  784,\n",
      "        1115,  827, 1403, 2718, 2149, 2430]), 25.361094342014606, tensor([ 799, 3289, 1054, 2896, 1153, 1489,  673, 2439, 2667,  103, 1112, 2438,\n",
      "        2477, 2957, 1897, 2327, 2142,  976, 2345, 2313, 1423, 2504, 2419, 1604,\n",
      "        2409, 1334, 2627, 1754,   17, 2410])], [8, 25.36117121587863, tensor([ 799,  103, 1210,   17, 2410,  385, 2560,  124,  917, 2477,  535,  827,\n",
      "         990,  417, 3016, 1185,  976, 1368,  522,  165, 2412, 2490, 3239, 1450,\n",
      "         223, 3121, 1394, 1054,   89, 2419]), 25.361163760313275, tensor([ 799,  103,  124, 2438, 3257,   17, 1185, 3289, 2490, 1468,  827,  546,\n",
      "         915,  556, 2766, 2603, 1754, 2788,  335, 3290, 2718,  223,  233, 3056,\n",
      "        3016,  796, 1782, 1809, 2680,  665])]]\n"
     ]
    }
   ],
   "source": [
    "###########如果不能为每个疾病找到特征基因，就反复点击这段代码，直到找到为止\n",
    "testset_xiaoguan  = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/test_info.csv')\n",
    "   \n",
    "test_loader_xiaoguan = DataLoader(\n",
    "         dataset=testset_xiaoguan,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True\n",
    "        \n",
    "     )\n",
    "#小管特征        \n",
    "for data, target in test_loader_xiaoguan:############小管\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    #targets=torch.max(targets,1)[1]###################\n",
    "    #print('data:',data.shape)\n",
    "    batch_size = int(data.size()[0])\n",
    "    #print('batch_size:',batch_size)#=20\n",
    "    evidence_for_class = []\n",
    "    #print(\"target:\",target.shape)\n",
    "    #print('target:',target[3])\n",
    "    # Overlay with noise \n",
    "    # data[0] += 0.25 * data[0].max() * torch.Tensor(np.random.randn(28*28).reshape(1, 28, 28))\n",
    "    #model_prediction, true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "\n",
    "    for i in range(9):#10类\n",
    "    # Unfortunately, we had some issue with freeing pytorch memory, therefore \n",
    "    # we need to reevaluate the model separately for every class.\n",
    "        model_prediction, input_relevance_values = inn_model.innvestigate2(in_tensor=data, rel_for_class=i,target=target)\n",
    "        evidence_for_class.append(input_relevance_values)\n",
    "    #print('input_relevance_values:',input_relevance_values.shape)\n",
    "    #print('evidence_for_class:',len(evidence_for_class))\n",
    "    evidence_for_class = np.array([elt.numpy() for elt in evidence_for_class])\n",
    "    #print('evidence_for_class:',evidence_for_class.shape)#[10,20,784]\n",
    "    for idx, example in enumerate(data):#batch 中的每一个样本\n",
    "        #print('example:',example.shape)\n",
    "        prediction = np.argmax(model_prediction.cpu().detach(), axis=1)#\n",
    "        #print('prediction[idx]:',prediction[idx])\n",
    "        #print(evidence_for_class[prediction[idx]][idx])\n",
    "        #fig, axes = plt.subplots(3, 5)\n",
    "        '''\n",
    "        fig.suptitle(\"Prediction of model: \" + str(prediction[idx]) + \"({0:.2f})\".format(\n",
    "            100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())))\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        prediction_value=prediction[idx]\n",
    "        p_x=model_prediction[idx][model_prediction[idx].argmax()].exp()\n",
    "        p_sum=model_prediction[idx].exp().sum()\n",
    "        prediction_score=100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())\n",
    "        #print('prediction_value:',prediction_value)\n",
    "        #print('prediction_score:',prediction_score)\n",
    "        #print('分子:',p_x)\n",
    "        #print('分母：',p_sum)\n",
    "        #uu=pr\n",
    "        #print(\"torch.argmax:\",torch.argmax(target[idx]))\n",
    "        if len(genes_features[prediction_value])==3:#有小球的值，但还没有添加小管预测分数和特征值，只有标签\n",
    "            if prediction_value!=torch.argmax(target[idx]).cpu().detach()  :\n",
    "                print('不合格****************:',prediction_value)\n",
    "            if prediction_value==torch.argmax(target[idx]).cpu().detach()  :#预测正确\n",
    "                genes_features[prediction_value].append(prediction_score)\n",
    "                print('合格：',prediction_value)\n",
    "                relevance_score_for_every_pixel=evidence_for_class[prediction[idx]][idx]\n",
    "                #print('relevance_score_for_every_pixel.shape:',relevance_score_for_every_pixel.tolist())\n",
    "                relevance_score_for_every_pixel=torch.from_numpy(relevance_score_for_every_pixel)\n",
    "                index=torch.topk(relevance_score_for_every_pixel,30,largest=True)#基因个数50\n",
    "                #print('pixel_sorted:',index)\n",
    "                genes_features[prediction_value].append(index.indices)#添加前50基因特征\n",
    "        \n",
    "        else: \n",
    "            if genes_features[prediction_value][3]<prediction_score:#如果值比已有值大，说明预测更准确\n",
    "                if prediction_value==torch.argmax(target[idx]).cpu().detach():\n",
    "                    print('&&&&&&&&&&&&&&&&&&&合   格&&&&&&&&&&&&&&:',prediction_value)\n",
    "                    genes_features[prediction_value].pop(4)#先删除特征值\n",
    "                    genes_features[prediction_value].pop(3)#先删除预测分数\n",
    "                \n",
    "                    genes_features[prediction_value].append(prediction_score)\n",
    "        \n",
    "                    relevance_score_for_every_pixel=evidence_for_class[prediction[idx]][idx]\n",
    "                    #print('relevance_score_for_every_pixel.shape:',relevance_score_for_every_pixel.tolist())\n",
    "                    relevance_score_for_every_pixel=torch.from_numpy(relevance_score_for_every_pixel)\n",
    "                    index=torch.topk(relevance_score_for_every_pixel,30,largest=True)#基因个数50\n",
    "                    #print('pixel_sorted:',index)\n",
    "                    genes_features[prediction_value].append(index.indices)\n",
    "    print('guan______________genes_features.shape:',genes_features)       \n",
    "        \n",
    "    '''\n",
    "        #写个算法，看多少个像素就能把一个图像全都表达出来\n",
    "        relevance_score_for_every_pixel=evidence_for_class[prediction[idx]][idx]\n",
    "        #relevance_score_for_every_pixel.sort()\n",
    "        row,=relevance_score_for_every_pixel.shape\n",
    "        num=0\n",
    "        for i in range(row):\n",
    "            if relevance_score_for_every_pixel[i,]<0.005:\n",
    "                evidence_for_class[prediction[idx]][idx][i,]=1\n",
    "            else:\n",
    "                num=num+1\n",
    "        print(\"num:\",num)       \n",
    "        #print('evi:',evidence_for_class[prediction[idx]][idx])\n",
    "        vmin = np.percentile(evidence_for_class[:, idx], 50)\n",
    "        vmax = np.percentile(evidence_for_class[:, idx], 99.9)\n",
    "        \n",
    "        axes[0, 2].imshow(example[0])\n",
    "        \n",
    "        axes[0, 2].set_title(\"Input (\" + str(int(target[idx]))+ \")\")\n",
    "        #axes[0, 3].imshow(evidence_for_class[prediction[idx]][idx][0], vmin=vmin,\n",
    "                          #vmax=vmax, cmap=\"hot\")#【0】\n",
    "        axes[0, 3].imshow(evidence_for_class[prediction[idx]][idx].reshape(28,28), vmin=vmin,\n",
    "                          vmax=vmax, cmap=\"hot\")\n",
    "        axes[0, 3].set_title(\"Pred. Evd.\")\n",
    "        for ax in axes[0]:\n",
    "            ax.set_axis_off()\n",
    "        for j, ax in enumerate(axes[1:].flatten()):\n",
    "            #im = ax.imshow(evidence_for_class[j][idx][0], cmap=\"hot\", vmin=vmin,\n",
    "                         # vmax=vmax)#【0】。reshape\n",
    "            im = ax.imshow(evidence_for_class[j][idx].reshape(28,28), cmap=\"hot\", vmin=vmin,\n",
    "                          vmax=vmax)\n",
    "            ax.set_axis_off()\n",
    "            ax.set_title(\"Evd. \" + str(j))\n",
    "        fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "        plt.show()\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    break\n",
    "        # fig.savefig(\"./mnist_example{0}.png\".format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e905a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 30])\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    \n",
    "    guan_genes_tensor=genes_features[i][4]\n",
    "    if i==0:\n",
    "        mm=guan_genes_tensor\n",
    "    else:\n",
    "        mm=torch.vstack((mm,guan_genes_tensor))\n",
    "print(mm.shape)\n",
    "guan_genes_numpy=mm.detach().numpy()\n",
    "guan_genes_df=pd.DataFrame(guan_genes_numpy)\n",
    "guan_genes_df.to_csv('dataset/qiuguan/origin_800/LRP/30/guan_genes_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df2903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "#保存筛选的特征基因\n",
    "\n",
    "import itertools\n",
    "\n",
    "qiu_guan_all=[]\n",
    "\n",
    "for i in range(9):\n",
    "    #print('i:',i)\n",
    "    qiu_features=genes_features[i][2]\n",
    "    guan_features=genes_features[i][4]\n",
    "    #print(qiu_features)\n",
    "    qiu_features=qiu_features.detach().numpy()\n",
    "    guan_features=guan_features.detach().numpy()\n",
    "    qiu_guan_features = list(set(itertools.chain(*[qiu_features,guan_features])))\n",
    "    #print(qiu_guan_features)\n",
    "    #qiu_guan_features.append(i)\n",
    "    qiu_guan_all.append(qiu_guan_features)\n",
    "#qiu_guan_all.append('label')\n",
    "#qiu_guan_all.append(10478)\n",
    "\n",
    "\n",
    "qiu_guan_all_np=np.array(qiu_guan_all)\n",
    "qiu_guan_all_df=pd.DataFrame(qiu_guan_all_np)\n",
    "qiu_guan_all_df.to_csv('dataset/qiuguan/origin_800/LRP/30/qiu_guan_all_selected_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8cc7a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 160)\n"
     ]
    }
   ],
   "source": [
    "#求出所有基因的个数，基因名数\n",
    "##########对所有疾病的特征基因求交集\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "f_genes='dataset/qiuguan/origin_800/LRP/30/qiu_guan_all_selected_features.csv'\n",
    "f_genes_df=pd.read_csv(f_genes,sep=',',header=None)\n",
    "#print(f_genes_df)\n",
    "f_genes_df=f_genes_df.iloc[1:,1:]\n",
    "#print(f_genes_df)\n",
    "#f_genes_df=f_genes_df.astype(int)\n",
    "#f_genes_df=f_genes_df.values\n",
    "\n",
    "qiu_guan_all=f_genes_df\n",
    "qiu_guan_all=np.array(qiu_guan_all)\n",
    "##qiu_guan_all.dtype='int'\n",
    "#print(qiu_guan_all)\n",
    "for i in range(9):\n",
    "    #print('i:',i)\n",
    "    qiu_guan_all_list=qiu_guan_all[i][0]\n",
    "    #qiu_guan=qiu_guan[:-1]\n",
    "    #print(qiu_guan)\n",
    "    #=qiu_guan_all_list[0]\n",
    "    qiu_guan_all_list_to_int = qiu_guan_all_list[1:-1]#去掉方括号，已经不是列表了，把方括号整体当作一个字符了\n",
    "    \n",
    "    qiu_guan_all_list_to_int=qiu_guan_all_list_to_int.split(',')#按，分割\n",
    "    qiu_guan_all_list_to_int=[int(j) for j in qiu_guan_all_list_to_int]\n",
    "    #print(qiu_guan_all_list_to_int)\n",
    "    #guan_features=genes_features[i][4]\n",
    "    #print(qiu_features)\n",
    "    #qiu_features=qiu_features.detach().numpy()\n",
    "    #guan_features=guan_features.detach().numpy()\n",
    "    #qiu_guan=[str(i) for i in qiu_guan]\n",
    "    if i==0:\n",
    "        qiu_guan_final=qiu_guan_all_list_to_int\n",
    "    else:\n",
    "        \n",
    "        #qiu_guan_final=qiu_guan_final.\n",
    "        qiu_guan_final = list(set(itertools.chain(*[qiu_guan_final,qiu_guan_all_list_to_int])))###########将球管合并，删除重复基因\n",
    "#print(len(qiu_guan_final))\n",
    "qiu_guan_final_len=len(qiu_guan_final)        \n",
    "\n",
    "        #print(qiu_guan_features)\n",
    "        #qiu_guan_features.append(i)\n",
    "        #qiu_guan_all.append(qiu_guan_final)\n",
    "#qiu_guan_all.append('label')\n",
    "#qiu_guan_all.append(10478)\n",
    "#print(qiu_guan_all)\n",
    "\n",
    "qiu_guan_final=np.array(qiu_guan_final).reshape(1,qiu_guan_final_len)\n",
    "print(qiu_guan_final.shape)\n",
    "qiu_guan_final=pd.DataFrame(qiu_guan_final)\n",
    "qiu_guan_final.to_csv('dataset/qiuguan/origin_800/LRP/30/qiu_guan_all_names_new_df_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3045e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all:             1                  2                  3                  4     \\\n",
      "0           RHOA               STX2              CISD1              WDR11   \n",
      "1    11.02328453        6.234847141        10.16119932        7.733762949   \n",
      "2    11.12296085        6.050914631        10.04924825        7.506624491   \n",
      "3    11.34208097  6.201381637999999        9.584881578  7.335857927999999   \n",
      "4     11.4064088        6.377802689  9.637117742000001        7.071728444   \n",
      "..           ...                ...                ...                ...   \n",
      "541       12.124            8.98472            8.99447             8.5349   \n",
      "542      12.7133            8.60548             8.8762            9.57229   \n",
      "543      12.2895            8.69831            9.33649             9.1396   \n",
      "544      12.2937            8.91005            8.67297            9.03033   \n",
      "545      12.4217            8.97756            8.72483            8.78966   \n",
      "\n",
      "            5            6            7            8            9     \\\n",
      "0          SCYL2      MGC2889       CCDC47         KLF8         CCL1   \n",
      "1    6.561803367  4.395615827  8.865512613  4.876233486  4.390281998   \n",
      "2    5.805898803  4.170993439  8.298354497  4.676167048  4.436150368   \n",
      "3    6.016254354  4.627032054  8.237579791  5.512879839  4.521375128   \n",
      "4    6.325214575  4.638218073  8.582068035  4.866330204  4.483266836   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "541      6.92251      5.24259      10.0701      6.02468      5.55907   \n",
      "542      8.05011      5.84873      10.5337      6.53178      5.52123   \n",
      "543      7.08377      5.48966      10.1373      6.21554       5.4477   \n",
      "544      6.54668      5.19375      9.68856      6.31567      5.46173   \n",
      "545      7.03525      5.47437      10.2083      6.15237      5.51313   \n",
      "\n",
      "                   10    ...                3293               3294  \\\n",
      "0               SLCO3A1  ...               PLAC4              NRBP1   \n",
      "1           4.627956778  ...  5.4060972210000005        7.059489579   \n",
      "2    5.0102396680000005  ...         5.174191982        7.164275442   \n",
      "3     5.976127287000001  ...          5.68539634         6.90335468   \n",
      "4           5.005922044  ...   5.773406972999999  7.459363142999999   \n",
      "..                  ...  ...                 ...                ...   \n",
      "541             7.92442  ...             6.83953            9.05786   \n",
      "542              8.2354  ...             7.17312             8.2466   \n",
      "543             7.21099  ...             6.69691            8.52336   \n",
      "544             7.67761  ...             6.70963             9.1295   \n",
      "545             7.76284  ...             6.88069            8.75566   \n",
      "\n",
      "                   3295         3296               3297               3298  \\\n",
      "0                LRRC23        SPHK2           KIAA0513            SULT1A1   \n",
      "1           6.141179114  6.941562277  5.626944062000001        9.436577809   \n",
      "2           6.221787327  7.367022385        5.727319784  8.494555178999999   \n",
      "3    5.8066373229999995  6.356903768  6.046559887999999         8.90178433   \n",
      "4           6.333961502  6.664118795  6.073709437000001        8.816798208   \n",
      "..                  ...          ...                ...                ...   \n",
      "541              6.3008      8.13566            7.30987            11.2973   \n",
      "542             5.97221      8.31733            7.05397            10.3732   \n",
      "543             6.45588       7.8646            7.72103            11.0422   \n",
      "544             6.52644      7.87535            7.39121            10.9655   \n",
      "545             6.16227       7.9012            7.37838            10.8738   \n",
      "\n",
      "                  3299         3300         3301   3302  \n",
      "0                 AMOT          CA1        GPR35  label  \n",
      "1          10.46253762  5.908204565   4.85878376      2  \n",
      "2          10.10077492   5.78523836  5.074548577      4  \n",
      "3          9.683268743    6.5124765  5.143392773      1  \n",
      "4    9.845285517999999  6.010021526  5.025494054      8  \n",
      "..                 ...          ...          ...    ...  \n",
      "541            12.8157      6.80576      5.76466      4  \n",
      "542            12.6145      7.24433      5.89702      8  \n",
      "543            13.3331      6.81914      5.69388      6  \n",
      "544            11.3957      6.73996      5.75556      5  \n",
      "545            12.7762      7.28409      5.86197      7  \n",
      "\n",
      "[546 rows x 3302 columns]\n",
      "shape: (546, 3302)\n",
      "all_xiaoguan:            1                   2                  3                   4     \\\n",
      "0          RHOA                STX2              CISD1               WDR11   \n",
      "1   10.43877345   6.206201062000001         9.83624556   7.038885936000002   \n",
      "2   10.75776176          5.78461146        10.15910478         7.212917031   \n",
      "3   10.73198235  5.8263509970000005        9.697981301         7.459396783   \n",
      "4   10.78846053         6.213282913        9.902440459         7.238626665   \n",
      "..          ...                 ...                ...                 ...   \n",
      "64  10.66485469   5.718776727000001        10.08128643   7.139878092000001   \n",
      "65   11.4532036  6.2127787020000005        9.751237432         7.298793834   \n",
      "66   11.4520551         6.136437787  9.835635027999999         7.622178822   \n",
      "67  11.24941046         6.091850169  9.683791771000001  7.6864780810000015   \n",
      "68  11.23359538   5.951661447999999        9.587977125         7.374923261   \n",
      "\n",
      "                  5                  6                  7     \\\n",
      "0                SCYL2            MGC2889             CCDC47   \n",
      "1          6.703748667        4.992982691        7.756204507   \n",
      "2          6.947931072        4.506135057  8.508243826000001   \n",
      "3          5.982515281  4.725451748999999        8.610901642   \n",
      "4           6.59562505        4.276114358        8.500638768   \n",
      "..                 ...                ...                ...   \n",
      "64         6.486716101  4.604959313999999        8.212338073   \n",
      "65         5.292133903        4.551763004        8.429374746   \n",
      "66   6.693718131000002         4.63755072        8.303779274   \n",
      "67  6.5145405589999985        4.339584175        8.735502146   \n",
      "68         6.083387154         4.65305322        8.667519634   \n",
      "\n",
      "                  8            9                   10    ...  \\\n",
      "0                 KLF8         CCL1             SLCO3A1  ...   \n",
      "1    5.403539242000001  4.953336836         4.975166755  ...   \n",
      "2          4.792093138  4.616944174  4.7756265330000005  ...   \n",
      "3          5.021616089  4.482129482         4.973848803  ...   \n",
      "4          5.046517831  4.423059063         4.887677755  ...   \n",
      "..                 ...          ...                 ...  ...   \n",
      "64  5.0210160010000005  4.876597328         5.390088652  ...   \n",
      "65   5.040017536000001  4.372635453         5.912838014  ...   \n",
      "66         4.872868799  4.489822578         5.366902622  ...   \n",
      "67         4.852528508  4.177709677         5.204577316  ...   \n",
      "68         5.228303384  4.423543877         5.398046158  ...   \n",
      "\n",
      "                 3293               3294                3295  \\\n",
      "0               PLAC4              NRBP1              LRRC23   \n",
      "1         5.909758275        7.399954359          6.61437644   \n",
      "2         5.622650798        7.374075117          6.69074083   \n",
      "3         5.896752025  7.194073142000001         6.497417771   \n",
      "4         5.534880109        7.192118785   6.731126257000001   \n",
      "..                ...                ...                 ...   \n",
      "64        5.914693187        6.540580638         6.403994604   \n",
      "65        5.446255346        7.190042561         5.789204369   \n",
      "66        5.626318691        7.039907359  6.0473376189999986   \n",
      "67  5.319871791000001        6.944893447         5.692401103   \n",
      "68        5.662448549        7.014004142          6.45868198   \n",
      "\n",
      "                  3296                3297               3298  \\\n",
      "0                SPHK2            KIAA0513            SULT1A1   \n",
      "1          7.272937691         5.729317966        7.674583514   \n",
      "2    7.390787482999999         5.591222011        8.241191458   \n",
      "3          7.430892975         5.896915715  8.175035452000001   \n",
      "4    7.515221627000001         5.657310992        9.043599461   \n",
      "..                 ...                 ...                ...   \n",
      "64   7.209390143999999         5.755793408        9.246369368   \n",
      "65   6.451511287000001  5.9419626810000015  8.954242772999999   \n",
      "66         6.980929009         5.954257996        8.834900242   \n",
      "67  6.6523954660000015         5.739934705        9.182997476   \n",
      "68         6.795951465         5.827040993        9.303854441   \n",
      "\n",
      "                 3299                3300               3301   3302  \n",
      "0                AMOT                 CA1              GPR35  label  \n",
      "1         10.48191009   6.607492357000001  5.084990073999999      5  \n",
      "2          10.2265216         5.914911025        4.968402034      5  \n",
      "3         10.63965394         6.074183635        5.161094451      8  \n",
      "4         10.45304126         5.812048805        4.867460722      6  \n",
      "..                ...                 ...                ...    ...  \n",
      "64        10.65093892         6.251332597        5.179551431      6  \n",
      "65        9.462974136   5.697293062999999        4.922467728      2  \n",
      "66        10.05264914  5.9509926470000005        5.079838802      7  \n",
      "67  9.876164253999999   5.723098672000001        4.641827546      8  \n",
      "68        10.21768988         6.429001678        4.992908698      1  \n",
      "\n",
      "[69 rows x 3302 columns]\n",
      "shape: (69, 3302)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_xiaoqiu:            1                  2            3                  4     \\\n",
      "0          RHOA               STX2        CISD1              WDR11   \n",
      "1   12.33147733        8.843523187  9.435926205        9.009858572   \n",
      "2    12.1801838  8.631155163999999  9.379102965  8.340756477000001   \n",
      "3   11.98135389  8.782033697000001  9.156410126         8.50322152   \n",
      "4   12.27318789        8.327898846  9.887579274  8.634472367999999   \n",
      "..          ...                ...          ...                ...   \n",
      "66  12.15324978        8.550678572  9.548931032        8.448183924   \n",
      "67  12.50080673        8.632233391   9.36522456        9.203636018   \n",
      "68  12.21194258        8.845552827  8.872258606        8.688882053   \n",
      "69  12.45519228        8.813248733  8.660720575        8.951006739   \n",
      "70  12.44740614        8.842976231  9.422864349            8.98144   \n",
      "\n",
      "                 5                  6            7                  8     \\\n",
      "0               SCYL2            MGC2889       CCDC47               KLF8   \n",
      "1         7.107385389        5.293767882  10.46077721        5.970377724   \n",
      "2   6.553368872999999         5.59496146  10.31891123  6.195326572000001   \n",
      "3         7.100889153  5.332064931000001  10.11469615        5.949817104   \n",
      "4         6.890654229        5.134498736  10.15167313        5.911585846   \n",
      "..                ...                ...          ...                ...   \n",
      "66  6.963025297000001        5.560785119  10.06171043  5.838032137999999   \n",
      "67        7.629995563        5.258812024  10.79060103        6.247449544   \n",
      "68        7.165948501        5.274344901  9.990952405         5.96229212   \n",
      "69        6.919345545  5.590528687000001  9.808096061        6.147042272   \n",
      "70         6.96336096        5.232355372  10.18005729        6.093205038   \n",
      "\n",
      "           9                   10    ...               3293  \\\n",
      "0          CCL1             SLCO3A1  ...              PLAC4   \n",
      "1   5.456362022         8.010685862  ...        6.545617018   \n",
      "2   5.148212623   7.918553597000001  ...        6.787745338   \n",
      "3    5.69811089  7.5962450010000016  ...        6.718973525   \n",
      "4   5.422055987         7.463196438  ...        6.638376928   \n",
      "..          ...                 ...  ...                ...   \n",
      "66  5.435461229          7.36761463  ...        6.817916155   \n",
      "67  5.101021978         7.697747482  ...  6.569031507999999   \n",
      "68  5.454456672          8.01294616  ...        6.687816671   \n",
      "69  5.336929136         7.800522306  ...        6.744503048   \n",
      "70   5.34422734            8.107972  ...  6.772365462000001   \n",
      "\n",
      "                 3294                3295               3296  \\\n",
      "0               NRBP1              LRRC23              SPHK2   \n",
      "1   8.789064568999999         6.501364154  8.106674291000001   \n",
      "2         8.647064104   6.287269157999999        8.058970028   \n",
      "3         8.343610583   6.578618917999999        8.220582121   \n",
      "4         8.646308241   6.760575467000001        8.624395779   \n",
      "..                ...                 ...                ...   \n",
      "66  8.516984111000001         6.326145945        8.170894134   \n",
      "67        8.373829021   6.509216492999999  7.947655532000001   \n",
      "68        8.576305928  6.2979280960000015  7.978046602999999   \n",
      "69        8.897809063   6.326778522000001        7.938734085   \n",
      "70        8.747005493         6.168588646        8.323464948   \n",
      "\n",
      "                  3297         3298         3299               3300  \\\n",
      "0             KIAA0513      SULT1A1         AMOT                CA1   \n",
      "1   7.3891213910000015   11.5854945  12.30578513        6.852594757   \n",
      "2          7.682618076  11.33812905  12.99551439        6.768580862   \n",
      "3          6.970958565  10.23805218  12.65000647  7.420241357999999   \n",
      "4          7.149147384  11.13805759  13.05792547        6.554545273   \n",
      "..                 ...          ...          ...                ...   \n",
      "66         7.434877934  11.70681344  12.59851799        6.781732082   \n",
      "67         7.205314355  10.60000609  12.97604296  8.518399068999999   \n",
      "68         7.433019937  10.75243783  12.60344335        6.951718447   \n",
      "69         7.661180541  11.71581567  12.73112788        7.054085336   \n",
      "70         7.353505928  11.46157698  13.06438276        6.790509884   \n",
      "\n",
      "                 3301   3302  \n",
      "0               GPR35  label  \n",
      "1         5.892521244      2  \n",
      "2         5.560514716      3  \n",
      "3         5.635434197      5  \n",
      "4         5.726263194      1  \n",
      "..                ...    ...  \n",
      "66         5.56679652      4  \n",
      "67        5.697897134      6  \n",
      "68  5.589825157000001      2  \n",
      "69  5.736978112999999      7  \n",
      "70         5.54816127      7  \n",
      "\n",
      "[71 rows x 3302 columns]\n",
      "shape: (71, 3302)\n",
      "column_names:    0     1     2     3     4    5     6    7     8    9    ...   151   152  \\\n",
      "0  NaN     0     1     2     3    4     5    6     7    8  ...   150   151   \n",
      "1  0.0  2560  2562  2054  1032  522  3086   17  2067  534  ...  3026  2011   \n",
      "\n",
      "   153  154   155  156   157   158   159  160  \n",
      "0  152  153   154  155   156   157   158  159  \n",
      "1  990  991  3042  487  3056  3061  1525  510  \n",
      "\n",
      "[2 rows x 161 columns]\n",
      "column_names:     1     2     3     4    5     6    7     8    9    10   ...   151   152  \\\n",
      "1  2560  2562  2054  1032  522  3086   17  2067  534  535  ...  3026  2011   \n",
      "\n",
      "   153  154   155  156   157   158   159  160  \n",
      "1  990  991  3042  487  3056  3061  1525  510  \n",
      "\n",
      "[1 rows x 160 columns]\n",
      "row_num,col_num: 1 160\n",
      "xxxxxxxxxxxxxxxx: [2560, 2562, 2054, 1032, 522, 3086, 17, 2067, 534, 535, 2077, 1054, 545, 546, 2603, 556, 3121, 1586, 2612, 572, 574, 2627, 1604, 1112, 89, 1115, 2140, 1117, 2142, 2658, 1125, 2149, 103, 1640, 2667, 1653, 2680, 124, 1151, 1153, 1156, 1669, 145, 1172, 1177, 665, 2718, 2208, 673, 1185, 165, 3239, 2218, 684, 1200, 180, 3257, 1210, 699, 2750, 707, 710, 198, 713, 716, 1230, 2766, 726, 3289, 3290, 1754, 223, 735, 2788, 1252, 233, 1779, 1782, 1272, 2296, 1787, 2304, 1793, 1798, 2313, 784, 1809, 277, 2838, 2327, 796, 799, 2345, 2349, 2862, 303, 1334, 1849, 827, 1855, 1870, 335, 2896, 1368, 1882, 1378, 2406, 1897, 2410, 2409, 2412, 2413, 1391, 880, 370, 2418, 2419, 1394, 1403, 2428, 2430, 385, 386, 2948, 2438, 2439, 2957, 1423, 2961, 915, 917, 2454, 1947, 417, 933, 2473, 1450, 2477, 950, 2490, 1468, 1470, 3008, 2497, 961, 2504, 3016, 460, 976, 1489, 3026, 2011, 990, 991, 3042, 487, 3056, 3061, 1525, 510]\n",
      "(546, 161)\n",
      "                   2561                2563               2055         1033  \\\n",
      "0                   TYR                 REN               LRP5       DNAJB1   \n",
      "1           4.732598858         7.815340608        5.126457596  8.806208475   \n",
      "2            4.89242217         5.574038933        5.285696071  8.828721231   \n",
      "3    5.1425137119999995  5.0302233560000005        4.517552114   8.38975821   \n",
      "4    5.2251349419999995         9.398038178  5.437829732999999  8.944571891   \n",
      "..                  ...                 ...                ...          ...   \n",
      "541             5.74353             10.4659            5.97308      8.04191   \n",
      "542             5.42785             9.65729            5.35862      7.48445   \n",
      "543             5.29333               10.86            5.44741      9.13281   \n",
      "544             6.07444             14.2059            5.08989       8.2727   \n",
      "545             5.19511             6.83467            5.21587      8.66892   \n",
      "\n",
      "                  523                3087                18    \\\n",
      "0                ABCD3              NUBPL                TFAM   \n",
      "1    9.659996297000001  5.552413377000001  6.1310359739999996   \n",
      "2          9.490747869        5.309114317   5.884019027999999   \n",
      "3          9.323142126        4.981802935         5.575372855   \n",
      "4          9.452798519        5.328429786   5.845830867999999   \n",
      "..                 ...                ...                 ...   \n",
      "541            8.91894            5.48593             6.31795   \n",
      "542            10.2675            5.32768             6.18701   \n",
      "543            9.72448            6.05422             5.91632   \n",
      "544            9.23821            5.78875             6.15767   \n",
      "545            8.96873            5.52444             6.13282   \n",
      "\n",
      "                   2068               535          536   ...  \\\n",
      "0                BDKRB2            ABHD17B        ABHD2  ...   \n",
      "1           6.273649466        6.907383976  5.868098357  ...   \n",
      "2            5.92375496        6.611677247  5.462809888  ...   \n",
      "3    6.2868099189999995  6.500580102000001  5.493090381  ...   \n",
      "4           5.966561614        6.731360611  6.088408821  ...   \n",
      "..                  ...                ...          ...  ...   \n",
      "541             5.85639            7.15251        5.779  ...   \n",
      "542             6.00474            7.09823      5.84687  ...   \n",
      "543             6.24121            6.66454      5.79904  ...   \n",
      "544             6.45041            6.96402      5.70836  ...   \n",
      "545             6.04916            6.82332      6.00481  ...   \n",
      "\n",
      "                  2012               991                992   \\\n",
      "0              TMEM100               KLF6           ATP6V0E2   \n",
      "1            4.8256251        8.502992246  8.370633207000001   \n",
      "2          4.302165691        7.515080496        9.012729871   \n",
      "3          4.832239922  8.470218167999999  7.587308577000001   \n",
      "4    4.561730481000001        8.416214013         8.77835302   \n",
      "..                 ...                ...                ...   \n",
      "541             4.8167            7.63777            9.58739   \n",
      "542            8.34999            7.54027             10.009   \n",
      "543            5.21359             8.1108            9.74936   \n",
      "544            4.81857                8.6            9.19305   \n",
      "545            6.70891            8.62633             9.0928   \n",
      "\n",
      "                   3043                488          3057         3062  \\\n",
      "0                  IGF1               AADAC      ATP5IF1     SERPINF1   \n",
      "1           4.905911145  4.7273019839999995  9.726076974   6.95045963   \n",
      "2           4.592165048   4.643809463999999  10.01457843  8.153844296   \n",
      "3           6.003111131         5.644519373  9.263673611  8.391644036   \n",
      "4    5.6790664060000005         5.074612076  9.390605009  7.551995811   \n",
      "..                  ...                 ...          ...          ...   \n",
      "541             7.26826             5.02497      10.0683      6.16271   \n",
      "542             6.95611             5.24354      10.5513      7.72591   \n",
      "543             7.94362             5.11613      10.2449      6.62334   \n",
      "544             8.29229             5.32283      10.4346       6.6727   \n",
      "545             7.67656             5.09261      9.68459       7.5329   \n",
      "\n",
      "                   1526               511    3302  \n",
      "0                MRPL44              ABCB7  label  \n",
      "1           5.938589097        7.523722018      2  \n",
      "2           6.055879486        7.920804834      4  \n",
      "3           5.464868312  7.042199062000001      1  \n",
      "4    5.8228072520000005  7.445341218999999      8  \n",
      "..                  ...                ...    ...  \n",
      "541             5.96811            7.76272      4  \n",
      "542             5.99075            8.03423      8  \n",
      "543             6.75146            7.54956      6  \n",
      "544             5.77096            7.80692      5  \n",
      "545             5.71518            7.18585      7  \n",
      "\n",
      "[546 rows x 161 columns]\n",
      "                  2561         2563                2055               1033  \\\n",
      "0                  TYR          REN                LRP5             DNAJB1   \n",
      "1          5.508282912  11.15897109          5.67133257        8.747716042   \n",
      "2           5.99758987  7.963371945         5.287103958        8.989019572   \n",
      "3          5.779268784  10.60184257          5.90649754        9.118595962   \n",
      "4          5.592153802  8.383229747   5.936658487000001        9.495621502   \n",
      "..                 ...          ...                 ...                ...   \n",
      "66         5.920123508  11.60109758         5.298572422        9.218327634   \n",
      "67  5.6146988439999985  12.93669403         5.553403178  8.421483842999999   \n",
      "68  5.4951580170000005   10.1560149  5.5271622539999985        8.838129077   \n",
      "69   5.456613602999999  10.84886105         5.442239627        8.691556237   \n",
      "70   5.459155742999999  9.509432017         5.657381571        8.343132068   \n",
      "\n",
      "                 523                 3087                18    \\\n",
      "0               ABCD3               NUBPL                TFAM   \n",
      "1         9.383546091         5.815439735   6.337579202000001   \n",
      "2         9.318897625         6.319717576         6.302930895   \n",
      "3          9.72067642         5.790358583         6.133593721   \n",
      "4         9.832125753  5.5418294470000005         6.246529289   \n",
      "..                ...                 ...                 ...   \n",
      "66        9.588654151         5.814006616         6.228094149   \n",
      "67        10.22120599         5.550020927         6.340505371   \n",
      "68  9.225470228999999         6.279770111   5.996536527000001   \n",
      "69        9.129667242          5.60057488         5.924999202   \n",
      "70        9.405897807   5.898259507000001  6.1863428460000005   \n",
      "\n",
      "                  2068               535                536   ...  \\\n",
      "0               BDKRB2            ABHD17B              ABHD2  ...   \n",
      "1          6.514949368        6.949135255  5.705720822000001  ...   \n",
      "2          6.543235304        6.986990092  5.831166992000001  ...   \n",
      "3          6.214442432        7.104531205        5.589718329  ...   \n",
      "4            7.0104997  7.003829152000001  5.658994752000001  ...   \n",
      "..                 ...                ...                ...  ...   \n",
      "66   6.421098992999999  7.004961237000001        5.844148952  ...   \n",
      "67   7.653440872000001         6.86921896  5.778527662999999  ...   \n",
      "68  6.2148602839999985        6.895724745        5.836217275  ...   \n",
      "69         5.944988332        6.788453941  5.754539692000001  ...   \n",
      "70         5.479314679        6.935989687  5.912916657999999  ...   \n",
      "\n",
      "                  2012               991          992                3043  \\\n",
      "0              TMEM100               KLF6     ATP6V0E2               IGF1   \n",
      "1          4.920976235        8.759145842  9.244744799        8.246970223   \n",
      "2           4.81999532        8.601619151  9.624427767        7.475480056   \n",
      "3          4.755806325        7.446008436  10.24392283        8.421346185   \n",
      "4   5.1064056760000005        7.801069054  10.59129803        7.221069924   \n",
      "..                 ...                ...          ...                ...   \n",
      "66          6.61274639  8.291233487000001  9.403866235        8.563108342   \n",
      "67   4.706926688999999        8.261687969  9.421678642  6.911460707000001   \n",
      "68         4.689085687          8.2939504  9.415674062        9.219731626   \n",
      "69   5.611330097000001  7.611571327999999   9.14952425        7.712924517   \n",
      "70         5.767091815         8.66032375  9.760843015        7.988235769   \n",
      "\n",
      "                  488                3057               3062  \\\n",
      "0                AADAC            ATP5IF1           SERPINF1   \n",
      "1          5.162019231        10.03931936        6.183482907   \n",
      "2   5.1891804310000005         9.94207942        7.875190506   \n",
      "3          5.217514822        10.37090648        6.352353442   \n",
      "4          4.996631913        10.56307072        7.834024883   \n",
      "..                 ...                ...                ...   \n",
      "66         5.204530004        9.844144814        7.565508768   \n",
      "67         5.022164507        10.14571148        10.16816718   \n",
      "68         5.048124036  9.910078287000001  6.007504782000001   \n",
      "69         5.129469006        9.740442003        6.431922598   \n",
      "70         5.038241816        10.04668945        8.200495191   \n",
      "\n",
      "                 1526         511    3302  \n",
      "0              MRPL44        ABCB7  label  \n",
      "1   6.111266542999999  7.420816759      2  \n",
      "2         6.168983207  7.387449993      3  \n",
      "3         6.351764219  7.974822662      5  \n",
      "4         6.423285092   8.08898091      1  \n",
      "..                ...          ...    ...  \n",
      "66  6.285141202999999  7.456950483      4  \n",
      "67        5.761998274  7.707365762      6  \n",
      "68        5.859333328  7.612995692      2  \n",
      "69        6.077673339  7.386504222      7  \n",
      "70        6.315931282  7.581683892      7  \n",
      "\n",
      "[71 rows x 161 columns]\n",
      "           2561               2563         2055               1033  \\\n",
      "0           TYR                REN         LRP5             DNAJB1   \n",
      "1   5.520969535  7.683289577999999  5.348692757        8.900220426   \n",
      "2   5.170211558        8.199240397  5.832398724        8.943453912   \n",
      "3   4.981713555        5.640395532  5.685261034        8.937418466   \n",
      "4   4.886192412        9.216987137  5.751148071  9.271156312999999   \n",
      "..          ...                ...          ...                ...   \n",
      "64  4.827506846        10.96623204   5.30784867  9.279455712999999   \n",
      "65  4.738063334  6.944704732999999  4.840550622        8.337073339   \n",
      "66  5.149915291        7.953917218  4.906228539        8.704887688   \n",
      "67  4.902856532        10.20491311  5.191411319  9.144718188999999   \n",
      "68  4.805637515        10.32993893  5.430559294        8.950586693   \n",
      "\n",
      "                 523                3087                18    \\\n",
      "0               ABCD3              NUBPL                TFAM   \n",
      "1         9.256031835        5.467139751         5.876359897   \n",
      "2   9.875904997000001  5.698118612999999         6.581598195   \n",
      "3          10.0790242        5.513927705         6.113708526   \n",
      "4         9.715913037        5.762159054         6.038464159   \n",
      "..                ...                ...                 ...   \n",
      "64        9.904506816        5.786200199         6.199437756   \n",
      "65        9.353956439        5.128230835         5.898275757   \n",
      "66        9.635080951        5.398054713         5.816890044   \n",
      "67        9.721811549        5.604472641  6.0335391320000005   \n",
      "68        9.601614278        5.662724825         5.785833191   \n",
      "\n",
      "                  2068               535                 536   ...  \\\n",
      "0               BDKRB2            ABHD17B               ABHD2  ...   \n",
      "1          5.833161139  7.254569742999999   5.792804532999999  ...   \n",
      "2          5.751526889        7.331464395   5.678784437000001  ...   \n",
      "3    6.307747753999998        6.685863491  5.9255965060000015  ...   \n",
      "4          6.149813729  6.945832427000001         5.799086468  ...   \n",
      "..                 ...                ...                 ...  ...   \n",
      "64         6.023928513        6.573091661   5.841652217999999  ...   \n",
      "65          6.11431522         6.40724282         5.758260383  ...   \n",
      "66         6.600183759         6.84243202  5.8991460610000015  ...   \n",
      "67         5.740206886  7.051542017999999   5.762619572999999  ...   \n",
      "68  6.3093315720000005        6.713221962         5.839432624  ...   \n",
      "\n",
      "                  2012               991          992                3043  \\\n",
      "0              TMEM100               KLF6     ATP6V0E2               IGF1   \n",
      "1          3.788932258         6.70310046  9.393130425        5.474307858   \n",
      "2          3.947383057        6.224869556  9.501896362        5.293864655   \n",
      "3   4.5069792710000005  7.346221257000001     8.960479        5.131227227   \n",
      "4          4.588720772        7.407573984  9.258464536        5.642244039   \n",
      "..                 ...                ...          ...                ...   \n",
      "64         4.478586677  7.419883402000001  9.467976933        5.433304027   \n",
      "65  4.3016198810000015         8.36724365  7.758547185        6.507139597   \n",
      "66         4.476665169  8.354928747999999  8.763524855  5.724187197000001   \n",
      "67         4.844528509         8.68361194  8.063491914        6.266415342   \n",
      "68         4.500838759  8.731108577999999  8.103957066        4.907554102   \n",
      "\n",
      "                  488          3057               3062               1526  \\\n",
      "0                AADAC      ATP5IF1           SERPINF1             MRPL44   \n",
      "1   5.0829741230000005  9.904613989        6.529160573  6.156156997999999   \n",
      "2          5.163118853  10.24118175        5.360009418        6.368152115   \n",
      "3            4.9746596  9.881105303        8.972016258        6.448974603   \n",
      "4          4.977489559  9.571311754        8.191478532        6.418324925   \n",
      "..                 ...          ...                ...                ...   \n",
      "64  4.4683001860000005   9.72812349         8.08634305        5.847800875   \n",
      "65         4.723933528  9.492799452         9.59809231        5.430163301   \n",
      "66         4.827475392  9.558887379        8.272011851        5.751402495   \n",
      "67         4.815573016  9.558755758  8.879658817000001         5.66245821   \n",
      "68         5.008358787  9.790187871          6.3897975         6.23774653   \n",
      "\n",
      "                  511    3302  \n",
      "0                ABCB7  label  \n",
      "1          7.870404731      5  \n",
      "2          7.756286827      5  \n",
      "3           7.79052721      8  \n",
      "4          8.110975192      6  \n",
      "..                 ...    ...  \n",
      "64         7.898127819      6  \n",
      "65          7.47864525      2  \n",
      "66         7.725588751      7  \n",
      "67         7.393249194      8  \n",
      "68  7.4402561139999985      1  \n",
      "\n",
      "[69 rows x 161 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "'''\n",
    "all=pd.read_csv('dataset/qiuguan/origin_800/new_df_df_800_non_encode.csv',sep=',',header=None)\n",
    "#all=pd.read_csv('dataset/qiuguan/orign/union_all_qiuguan.csv',sep=',')\n",
    "#print(all)\n",
    "#all.rename(columns={'labels.1': 'labels'}, inplace=True)\n",
    "all=all.iloc[:,1:]\n",
    "print('all:',all)\n",
    "print('shape:',all.shape)\n",
    "#print(all)\n",
    "#prin\n",
    "\n",
    "#X=X.values\n",
    "all_rows,all_cols=all.shape\n",
    "\n",
    "'''\n",
    "\n",
    "#######训练集 ：合起来：一部分\n",
    "all_qiuguan=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv',sep=',',header=None)\n",
    "#all=pd.read_csv('dataset/qiuguan/orign/union_all_qiuguan.csv',sep=',')\n",
    "#print(all)\n",
    "#all.rename(columns={'labels.1': 'labels'}, inplace=True)\n",
    "all_qiuguan=all_qiuguan.iloc[:,1:]\n",
    "print('all:',all_qiuguan)\n",
    "print('shape:',all_qiuguan.shape)\n",
    "#print(all)\n",
    "#prin\n",
    "\n",
    "#X=X.values\n",
    "all_rows,all_cols=all_qiuguan.shape\n",
    "\n",
    "###########测试集：分两部分：小球和小管\n",
    "\n",
    "all_xiaoguan=pd.read_csv('dataset/qiuguan/origin_800/xiaoguan/test_info.csv',sep=',',header=None)\n",
    "#all=pd.read_csv('dataset/qiuguan/orign/union_all_qiuguan.csv',sep=',')\n",
    "#print(all)\n",
    "#all.rename(columns={'labels.1': 'labels'}, inplace=True)\n",
    "all_xiaoguan=all_xiaoguan.iloc[:,1:]\n",
    "print('all_xiaoguan:',all_xiaoguan)\n",
    "print('shape:',all_xiaoguan.shape)\n",
    "#print(all)\n",
    "#prin\n",
    "\n",
    "#X=X.values\n",
    "#all_rows,all_cols=all.shape\n",
    "\n",
    "\n",
    "#all_rows,all_cols=all.shape\n",
    "\n",
    "###########测试集：分两部分：小球和小管\n",
    "\n",
    "all_xiaoqiu=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu/test_info.csv',sep=',',header=None)\n",
    "#all=pd.read_csv('dataset/qiuguan/orign/union_all_qiuguan.csv',sep=',')\n",
    "#print(all)\n",
    "#all.rename(columns={'labels.1': 'labels'}, inplace=True)\n",
    "all_xiaoqiu=all_xiaoqiu.iloc[:,1:]\n",
    "print('all_xiaoqiu:',all_xiaoqiu)\n",
    "print('shape:',all_xiaoqiu.shape)\n",
    "\n",
    "\n",
    "\n",
    "#all_rows_name=all.index=['a'+str(i) for i in range(all_rows)]\n",
    "#print(rows_name)\n",
    "\n",
    "#print(all)\n",
    "column_name=pd.read_csv('dataset/qiuguan/origin_800/LRP/30/qiu_guan_all_names_new_df_1.csv',sep=',',header=None)\n",
    "print('column_names:',column_name)\n",
    "column_name=column_name.iloc[1:,1:]\n",
    "row_num,col_num=column_name.shape\n",
    "print('column_names:',column_name)\n",
    "\n",
    "print('row_num,col_num:',row_num,col_num)\n",
    "#=column_name.astype(int)\n",
    "column_name=np.array(column_name).reshape(col_num).tolist()\n",
    "print('xxxxxxxxxxxxxxxx:',column_name)\n",
    "#column_name_index=[i-1 for i in column_name]############################################\n",
    "column_name_index=[i for i in column_name]\n",
    "#column_name[-1]='10478'\n",
    "#print('xxxxxxxxxxxxxxxx:',column_name)\n",
    "#column_name=[int(i) for i in range(column_name)]\n",
    "#column_name=list(map(int,column_name))\n",
    "#print('column_names:',column_name)\n",
    "#rows_name,col_name=column_name.shape\n",
    "#column_name=column_name.values.reshape(rows_name).tolist()\n",
    "column_name_index.append(3301)\n",
    "#all=all.loc[all_rows_name,column_name]\n",
    "all_qiuguan=all_qiuguan.iloc[:,column_name_index]\n",
    "print(all_qiuguan.shape)\n",
    "print(all_qiuguan)\n",
    "\n",
    "all_xiaoguan=all_xiaoguan.iloc[:,column_name_index]\n",
    "all_xiaoqiu=all_xiaoqiu.iloc[:,column_name_index]\n",
    "print(all_xiaoqiu)\n",
    "print(all_xiaoguan)\n",
    "'''\n",
    "X=all.iloc[1:,:-1]\n",
    "#print(column_name)\n",
    "\n",
    "#X=all.iloc[]\n",
    "\n",
    "#X.to_csv('dataset/qiuguan/origin_800/final_100/X.csv')\n",
    "#print(X)\n",
    "\n",
    "y=all.iloc[1:,-1]\n",
    "y=pd.DataFrame(y,dtype=int)\n",
    "\n",
    "'''\n",
    "\n",
    "#print(y)\n",
    "#print(y.dtype)\n",
    "#y=y.values\n",
    "\n",
    "#print(y)\n",
    "\n",
    "\"\"\"\n",
    "y=pd.read_csv('dataset/gene_247/data/guan/guan_label.csv',sep=',')\n",
    "y=y.values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "#y=np.array(y)\n",
    "train_val_data,test_data,train_val_label,test_label=train_test_split(X,y,test_size=0.2,stratify=y,random_state=3)\n",
    "\n",
    "#print(train_val_data)\n",
    "train_val_info=pd.concat([train_val_data,train_val_label],axis=1)\n",
    "test_info=pd.concat([test_data,test_label],axis=1)\n",
    "\n",
    "print(train_val_info)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_qiuguan.to_csv('dataset/qiuguan/origin_800/LRP/30/selected_train_val_info.csv')\n",
    "all_xiaoguan.to_csv('dataset/qiuguan/origin_800/LRP/30/selected_xiaoguan_test_info.csv')\n",
    "all_xiaoqiu.to_csv('dataset/qiuguan/origin_800/LRP/30/selected_xiaoqiu_test_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5cd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
