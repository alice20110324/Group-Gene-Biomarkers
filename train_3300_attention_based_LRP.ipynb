{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "input_num=3300\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    \n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "    \n",
    "    \n",
    "\n",
    "#model1 = MLP().cuda()\n",
    "#print(model1)\n",
    "\n",
    "class MLP_P(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3)    \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3) \n",
    "        \n",
    "        #self.model1=MLP1().cuda() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        y1=self.bn0(x)\n",
    "        y1 = F.relu(self.drop(self.bn1(self.fc1(y1))))\n",
    "        y1= F.relu(self.drop(self.bn2(self.fc2(y1))))\n",
    "        return F.softmax(self.bn3(self.fc3(y1)), dim=1)\n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "model = MLP().cuda()\n",
    "\n",
    "for i in model.state_dict():\n",
    "    print(i)\n",
    "print(model)\n",
    "mlp_paras=list(model.named_parameters())\n",
    "#print(mlp_paras)\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",                              beta=.5)\n",
    "\n",
    "#model1=model\n",
    "inn_model1 = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[:,1:]\n",
    "        \n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        \n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            \n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            \n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "       \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "def standN(x):\n",
    "    row,col=x.shape\n",
    "    max_x=torch.max(x,1)\n",
    "    min_x=torch.min(x,1)\n",
    "    #print(max_x)\n",
    "    #print(min_x)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-min_x.values[i])/max_x.values[i]\n",
    "    return x\n",
    "\n",
    "def standNorm(x):\n",
    "    row,col=x.shape\n",
    "    mean=torch.mean(x,1)\n",
    "    std=torch.std(x,1)\n",
    "    #print(mean)\n",
    "    #print(std)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-mean[i])/std[i]\n",
    "    return x\n",
    "\n",
    "def train_epoch(kii,epoch,train_loader,batch_size,optimizer,loss_func,loss2_list):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    #oss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    BL=nn.BatchNorm1d(input_num)\n",
    "    BL=BL.cuda()\n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        #xm=x.clone()\n",
    "        xm=x.clone().detach().cuda()\n",
    "        xn=x.clone().detach().cuda()\n",
    "        #loss2=0\n",
    "        if kii>=0: \n",
    "            \n",
    "            #inn_model2.evaluate(in_tensor=xn)\n",
    "            #inn_model1.evaluate(in_tensor=xm)\n",
    "            \n",
    "            model_prediction, only_max_score,org_shape = inn_model.innvestigatex(in_tensor=x)\n",
    "            model_prediction.cuda()\n",
    "            \n",
    "            #model_prediction1, only_max_score1,org_shape1= inn_model1.innvestigatex()\n",
    "            #model_prediction1.cuda()\n",
    "            \n",
    "            #only_max_score1=only_max_score.clone().detach()\n",
    "            \n",
    "            \n",
    "            #print('only_max_score:',only_max_score)\n",
    "            #only_max_score1=only_max_score.detach().clone()\n",
    "            #print(labels.shape,labels)   \n",
    "        \n",
    "            #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "            #print(model_prediction.shape, model_prediction)\n",
    "            #model_prediction.cuda()\n",
    "            #print('input_relevance_values:')\n",
    "            rel_for_class_list=labels\n",
    "            rel_for_class_list1=labels\n",
    "            para_list=[]\n",
    "            para_orign_list=[0]*batch_size\n",
    "            #print('inn_model1')\n",
    "            #input_relevance_values1,layers1=inn_model2.compute_relevance_scorex(only_max_score1,rel_for_class_list1,org_shape,para_orign_list)\n",
    "            ##input_relevance_values1,layers1=inn_model2.compute_relevance_scorex(only_max_score1,rel_for_class_list1,org_shape,para_orign_list)\n",
    "            \n",
    "            \n",
    "            for i in range(batch_size):\n",
    "            \n",
    "                max_pre=torch.argmax(model_prediction[i])\n",
    "                if max_pre!=labels[i]:\n",
    "                    #print('only_max_score1:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                    #rel_for_class=labels[i]\n",
    "                    para=random.gauss(c1,c2)#(kii>0,93.75)(ki>1,92.1875)\n",
    "                    #para=random.gauss(0.6,0.2)#96.8  92.2\n",
    "                    #para=0.6#(kii>0:93.75;>=0:89.0625)\n",
    "                    #para=0.4(kii>0,93.75)\n",
    "                    \n",
    "                    #para=random.uniform(0.3,0.8)#(kii=0,89.0625)\n",
    "                    para=round(para,4)\n",
    "                    #para=0.2\n",
    "                    para_list.append(para)\n",
    "                    #print('device:',input_relevance_values.device)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                    #print('pre_target:',pre_target[i])\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                else:\n",
    "                    #print('only_max_score2:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                    para=0\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "            #print(para_list)\n",
    "            #print(para_orign_list)\n",
    "            input_relevance_values,layers=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "            #print('inn_model2')\n",
    "            #input_relevance_values,input_relevance_values1,layers,layers1=inn_model2.compute_relevance_scorey(only_max_score,only_max_score1,rel_for_class_list,org_shape,para_list,para_orign_list)\n",
    "            #input_relevance_values=input_relevance_values.exp()############\n",
    "            #input_relevance_values1=input_relevance_values1.exp()############\n",
    "            \n",
    "            \n",
    "            #input_relevance_values1=input_relevance_values1[-1]\n",
    "            input_relevance_values=input_relevance_values[-1]\n",
    "            #sum_input_relevance_values=torch.sum(input_relevance_values)\n",
    "            #sum_input_relevance_values1=torch.sum(input_relevance_values1)\n",
    "            \n",
    "            \n",
    "            #input_relevance_values=F.softmax(input_relevance_values/sum_input_relevance_values)\n",
    "            #input_relevance_values1=F.softmax(input_relevance_values1/sum_input_relevance_values1)\n",
    "            \n",
    "            input_relevance_values=input_relevance_values.exp()############\n",
    "            #input_relevance_values1=input_relevance_values1.exp()\n",
    "            \n",
    "            #loss2=torch.abs(input_relevance_values1-input_relevance_values)\n",
    "            #loss2=torch.sum(loss2)\n",
    "            \n",
    "            \n",
    "            #loss2_list.append(loss2)\n",
    "            \n",
    "            ee=[[0]*input_num]*batch_size\n",
    "            ee=torch.tensor(ee,dtype=torch.float)\n",
    "        \n",
    "            ee=ee.cuda()\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                        max_pre=torch.argmax(model_prediction[i])######\n",
    "                        if max_pre!=labels[i]: #and i==min_predict_mis:###\n",
    "                            \n",
    "                                \n",
    "            \n",
    "                                ee[i]=torch.mul(x[i],input_relevance_values[-1][i])##############注意力33333333333333333weight\n",
    "                                #dd=torch.mul(bias,input_relevance_values[-1][i])############bias\n",
    "                                #print('cc:',i,cc)\n",
    "                                #ee[i]=torch.mul(x[i],input_relevance_values[-1][i])\n",
    "                                ################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ee=torch.mul(x,input_relevance_values[-1])\n",
    "            \n",
    "            x=torch.add(x,ee)\n",
    "            '''\n",
    "            ee=torch.mul(x,input_relevance_values)\n",
    "            #print('ee:',ee)\n",
    "            uu=F.softmax(ee/torch.sum(ee))\n",
    "            #print('uu:',uu)\n",
    "            x=torch.add(x,uu)\n",
    "            \n",
    "            '''\n",
    "           \n",
    "            #print('x:',x)\n",
    "            \n",
    "            '''\n",
    "            input_len=len(input_relevance_values)\n",
    "            #input_relevance_values1,layers1=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_orign_list)\n",
    "            ee=[[0]*input_num]*batch_size\n",
    "            ee=torch.tensor(ee,dtype=torch.float)\n",
    "        \n",
    "            ee=ee.cuda()\n",
    "            #sum_input_relevance_values=torch.sum(input_relevance_values,dim=0)\n",
    "            #input_relevance_values=F.softmax(input_relevance_values/sum_input_relevance_values,dim=1)\n",
    "            input_relevance_values[-1]=F.softmax(input_relevance_values[-1],dim=1)################\n",
    "        \n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                        max_pre=torch.argmax(model_prediction[i])######\n",
    "                        if max_pre!=labels[i]: #and i==min_predict_mis:###\n",
    "                            \n",
    "                                \n",
    "            \n",
    "                                ee[i]=torch.mul(x[i],input_relevance_values[-1][i])##############注意力33333333333333333weight\n",
    "                                #dd=torch.mul(bias,input_relevance_values[-1][i])############bias\n",
    "                                #print('cc:',i,cc)\n",
    "                                #ee[i]=torch.mul(x[i],input_relevance_values[-1][i])\n",
    "                                ################\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #ee=torch.mul(x,input_relevance_values[-1])\n",
    "            \n",
    "            if epoch%3==0:\n",
    "                x=torch.subtract(x,ee)\n",
    "            else:\n",
    "                \n",
    "                x=torch.add(x,ee)\n",
    "        \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            #x=torch.add(x,ee)\n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "              \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "        #print('x_new:',x)\n",
    "        #print('new_x:',x)\n",
    "        optimizer.zero_grad()\n",
    "        y_predict=model(x)\n",
    "        #y_predict=model(x,labels)\n",
    "            \n",
    "        \n",
    "        \n",
    "        loss1 = loss_func(y_predict, labels)\n",
    "        #print('loss1:',loss1)\n",
    "        #loss2=loss_func(y_predict1,labels)\n",
    "        #loss2=u*(1/loss_op)\n",
    "        #print('input_relevance_values:',input_relevance_values.shape)\n",
    "        #print('loss_score:',loss_score.shape)\n",
    "        #cc=1.0/torch.abs(torch.sum(cc))\n",
    "        \n",
    "        #print('loss1:',loss1)\n",
    "        #print('loss2:',loss2)\n",
    "        \n",
    "        \n",
    "        #loss2=u*cc\n",
    "        #print('input_relevance_values:',input_relevance_values)\n",
    "        \n",
    "        #print('loss1:',loss1)\n",
    "        #print('loss2:',loss2)\n",
    "        #loss=loss1+loss2\n",
    "        \n",
    "        #u=10000\n",
    "        #loss=loss1+u*loss2\n",
    "        loss=loss1\n",
    "        #loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss2_list.append(u*loss2)   \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "    #plotLoss(loss2_list,batch_idx+1)   #################################     \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy,loss2_list\n",
    "\n",
    "def val_epoch(test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "        print(\"val\")\n",
    "            \n",
    "            \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)     \n",
    "           \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()\n",
    "        targets=torch.max(targets,1)[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #yhat = model1(inputs,targets)\n",
    "        yhat=model(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "        total_test_acc+=batch_test_acc\n",
    "            \n",
    "        batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3114f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_,model=MLPA().cuda\n",
    "'''\n",
    "inn_model = InnvestigateModel(model1, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "'''\n",
    "import itertools\n",
    "num=0\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "loss2_list=[]\n",
    "for ki in range(K):\n",
    "    trainset = KZDataset(csv_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv',K=K, n_class=nfm_config['n_class'],ki=ki,  typ='train', transform=None, rand=True)\n",
    "    valset = KZDataset(csv_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv', K=K,n_class=nfm_config['n_class'],ki=ki,  typ='val', transform=None, rand=True)\n",
    "    train_loader = data.DataLoader(\n",
    "         dataset=trainset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True)\n",
    "    val_loader = data.DataLoader(\n",
    "         dataset=valset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "    \n",
    "    model_path='dataset/qiuguan/origin_800/non_encode_attention/new/para0.4_0.2/'\n",
    "    #BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    #total = 0\n",
    "    \n",
    "    \n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "   \n",
    "    model_named_parameters=[j for j in model.named_parameters()]\n",
    "    #print('model_parameters:',model_named_parameters)\n",
    "    #print('model.state.dict:',model.state_dict())\n",
    "    epoches=101\n",
    "    for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        train_loss_total,acc_train,loss2_list=train_epoch(ki,epoch_id,train_loader,nfm_config['batch_size'],optimizer,loss_func,loss2_list)\n",
    "        train_loss_total_list.append(train_loss_total)#\n",
    "        if epoch_id %10==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "    print(\"the \",ki,\" epoch ends\")\n",
    "    plotLoss(train_loss_total_list,epoches)\n",
    "    \n",
    "    train_loss_total_list=[]\n",
    "    acc_test=val_epoch(val_loader,nfm_config['batch_size'],optimizer)\n",
    "    print(\"acc_test_each_k:\",acc_test)\n",
    "    test_metrics.append(acc_test)\n",
    "\n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics)\n",
    "\n",
    "#print(list(mlp.named_parameters()))\n",
    "\n",
    "path=os.path.join(model_path,'MLP'+str(num)+str(12345)+'.pkl')\n",
    "torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff419dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotAcc(len_list,acc_score_list):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    #x=[i for i in range(lenth)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(len_list, acc_score_list, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()\n",
    "    \n",
    "testset_guan = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/test_info.csv')\n",
    "   \n",
    "test_loader_guan = data.DataLoader(\n",
    "         dataset=testset_guan,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "testset_qiu = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoqiu/test_info.csv')\n",
    "   \n",
    "test_loader_qiu = data.DataLoader(\n",
    "         dataset=testset_qiu,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "input_num=3300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################149特征基因运行后的示意图\n",
    "################################################特征基因个数为300\n",
    "\n",
    "##############小球测试\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "#path='dataset/qiuguan/origin_800/LRP/non_encode/40/attention0/MLP1112345.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "#print(model.state_dict())\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(mlp_params)\n",
    "net=mlp\n",
    "\n",
    "\n",
    "#net=model\n",
    "\n",
    "testset_guan = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/test_info.csv')\n",
    "   \n",
    "test_loader_guan = data.DataLoader(\n",
    "         dataset=testset_guan,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "testset_qiu = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoqiu/test_info.csv')\n",
    "   \n",
    "test_loader_qiu = data.DataLoader(\n",
    "         dataset=testset_qiu,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eba40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PlotCurves import plotGraph,plotLoss,plotMatrix\n",
    "from train_val_test import evaluate_model\n",
    "import os\n",
    "path_dir='dataset/qiuguan/origin_800/non_encode_attention/new/para0.4_0.2/'\n",
    "mlp=MLP()\n",
    "path_list=[]\n",
    "acc_score_list=[]\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score \n",
    "pos=0\n",
    "for parent, _, files in os.walk(path_dir):\n",
    "    pathes=files\n",
    "for path_ in pathes:\n",
    "    if path_.startswith('MLP'):  # 去除隐藏文件\n",
    "\n",
    "\n",
    "        \n",
    "        path=os.path.join(path_dir,path_)\n",
    "        print(path)\n",
    "        mlp.load_state_dict(torch.load(path),strict=False)\n",
    "        mlp.cuda()\n",
    "\n",
    "        net=mlp\n",
    "        actuals,predictions,acc_test=evaluate_model(test_loader_guan,net,input_num)\n",
    "\n",
    "       \n",
    "        target_list=actuals \n",
    "        pred_list=predictions \n",
    "      \n",
    "        y_true=target_list \n",
    "        y_pred=pred_list \n",
    "      \n",
    "\n",
    "        acc_score=accuracy_score(y_true=target_list, y_pred=pred_list) \n",
    "\n",
    "        acc_score_list.append(acc_score)\n",
    "        path_list.append(path)\n",
    "        pos=pos+1\n",
    "max_guan=max(acc_score_list)\n",
    "print(max_guan)\n",
    "#max_len=len(max_qiu)\n",
    "max_guan_list=[]\n",
    "max_guan_len_list=[]\n",
    "for i,score in enumerate(acc_score_list):\n",
    "    \n",
    "    if score==max_guan:\n",
    "        print(i,path_list[i],score)\n",
    "        \n",
    "        max_guan_list.append(score)\n",
    "        max_guan_len_list.append(i)\n",
    "#lenth=len(acc_score_list)\n",
    "\n",
    "plotAcc(max_guan_len_list,max_guan_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
